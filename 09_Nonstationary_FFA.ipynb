{"cells":[{"cell_type":"markdown","metadata":{"id":"_prLEFaiCrNz"},"source":["# Non-stationary flood frequency analysis using the LN2 and LN3 distributions estimated with MOM or MLE"]},{"cell_type":"markdown","metadata":{"id":"LErNDHg1EF_H"},"source":["Install lmoments3 and dataretrieval libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cud2Z9k5CqMI"},"outputs":[],"source":["!pip install dataretrieval\n","!pip install lmoments3"]},{"cell_type":"markdown","metadata":{"id":"xn-dZOpoEEOx"},"source":["Load utils.py from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQwKblbIC3lX"},"outputs":[],"source":["from google.colab import drive\n","import numpy as np\n","import scipy.stats as ss\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import dataretrieval.nwis as nwis\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","import seaborn as sns\n","\n","# allow access to google drive\n","drive.mount('/content/drive')\n","\n","!cp \"drive/MyDrive/Colab Notebooks/CE6280/CodingExamples/utils.py\" .\n","from utils import *"]},{"cell_type":"markdown","metadata":{"id":"eU9h0N7CECm_"},"source":["Consider the time series of annual maxima at Whiteoak Bayou in Houston, TX from water years 1936-2024. This is USGS site [08074500](https://waterdata.usgs.gov/nwis/inventory?site_no=08074500&agency_cd=USGS)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2mArznNEDhh"},"outputs":[],"source":["flow_df = nwis.get_record(sites='08074500', service='peaks', start='1935-10-01', end='2024-09-30') # Whiteoak Bayou in Houston, TX\n","fig, ax = plt.subplots(1,1)\n","flow_df['peak_va'].plot(ax=ax,label=\"Annual Maxima\")\n","ax.axhline(np.mean(flow_df['peak_va']),color=\"tab:red\",label=\"Mean\")\n","handles, labels = ax.get_legend_handles_labels()\n","ax.legend(handles,labels,loc=\"upper left\")"]},{"cell_type":"markdown","metadata":{"id":"O9zNgSCnE8ak"},"source":["## Stationary flood-frequency analysis\n","\n","It looks like this time series is non-stationary, so assuming stationarity could lead to underestimation of flood risk. Let's see what that estimated risk would be if we fit LN2 and LN3 distributions to the annual maxima using MOM, MLE, and Lmom assuming stationarity. The code for this is in utils.py.\n","\n","What would be the estimated return period of the highest observed flood under this assumption?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ci_I5dcvFG1l"},"outputs":[],"source":["methods = [\"MOM\", \"MLE\", \"Lmom\"]\n","npars = [2, 3]\n","flow_df['Year'] = range(1936,2025)\n","\n","for method in methods:\n","  for npar in npars:\n","    distfit = LogNormal()\n","    distfit.fit(flow_df['peak_va'],method, npar, initialize=False)\n","    q100 = distfit.findReturnPd(100)\n","    Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), distfit.sigma, distfit.tau, np.exp(distfit.mu)))\n","    distfit.plotHistPDF(flow_df['peak_va'], 0, 51000, \"LN\" + str(npar) + \" \" + str(method) + \" Fit\")\n","    l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","    l2, = plt.plot(flow_df[\"Year\"], np.ones(len(flow_df[\"Year\"]))*q100, color=\"tab:red\")\n","    plt.title(\"Stationary LN\" + str(npar) + \" Fit\")\n","    plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","    plt.show()\n","    print(\"LN%d %s mu: %0.2f\" % (npar, method, distfit.mu))\n","    print(\"LN%d %s sigma: %0.2f\" % (npar, method, distfit.sigma))\n","    print(\"LN%d %s tau: %0.2f\" % (npar, method, distfit.tau))\n","    print(\"LN%d %s 100-yr flood: %0.0f cms\" % (npar, method, q100))\n","    print(\"Return period of largest flood under LN%d %s fit: %0.0f year\" % (npar, method, Tmax))\n","    print(\"\\n\")"]},{"cell_type":"markdown","source":["The error when fitting LN3 with L-moments indicates we need to expand the bound in our root finding because the current bounds do not span a root. We would have to change this in utils.py.\n","\n","Note: there is a better way to find a bracket that will span zero with [scipy.optimize.elementwise.bracket_root](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.elementwise.bracket_root.html#scipy.optimize.elementwise.bracket_root) but that function is only available in scipy v1.15. We are using scipy v1.14 because that is the latest version compatible with Python 3.11, which is the default version in Google Colab. Upgrading the Python version in Google Colab is a pain, but if you want to do it on your own machine, it is easier. Example code using scipy.optimize.elementwise.bracket_root within the LogNormal.fit function is shown below if you'd like to do that.\n","\n","```\n","from scipy.optimize.elementwise import bracket_root\n","\n","class LogNormal(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","    self.sigma = None\n","    self.tau = None\n","\n","  def fit(self, data, method, npars, initialize=True):\n","    assert method == 'MLE' or method == 'MOM' or method == \"Lmom\",\"method must = 'MLE','MOM', or 'Lmom'\"\n","    assert npars == 2 or npars == 3,\"npars must = 2 or 3\"\n","\n","    self.findMoments(data)\n","    self.findLmoments(data)\n","    if method == 'MLE':\n","      if initialize == False:\n","        if npars == 2:\n","          shape, loc, scale = ss.lognorm.fit(data, floc=0)\n","        elif npars == 3:\n","          shape, loc, scale = ss.lognorm.fit(data)\n","      else:\n","        if npars == 2:\n","          self.fit(data, 'Lmom', 2)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma, floc=0)\n","        elif npars == 3:\n","          self.fit(data, 'Lmom', 3)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma)\n","\n","      self.mu = np.log(scale)\n","      self.sigma = shape\n","      self.tau = loc\n","    elif method == 'MOM':\n","      if npars == 2:\n","        self.sigma = np.sqrt(np.log(1+self.var/self.xbar**2))\n","        self.mu = np.log(self.xbar) - 0.5*self.sigma**2\n","        self.tau = 0\n","      elif npars == 3:\n","        bracket = bracket_root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                    xl0=0.01)\n","        self.sigma = root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                   bracket.bracket[0], bracket.bracket[1])\n","        self.mu = 0.5 * (np.log(self.var / (np.exp(self.sigma**2)-1)) - self.sigma**2)\n","        self.tau = self.xbar - np.exp(self.mu + 0.5*self.sigma**2)\n","    elif method == 'Lmom':\n","      if npars == 2:\n","        self.tau = 0\n","        self.sigma = root(lambda x: special.erf(x/2) - self.L2/self.L1, 0.01, self.L2)\n","        self.mu = np.log(self.L1) - 0.5*self.sigma**2\n","      elif npars == 3:\n","        bracket = bracket_root(lambda x: self.T3 - (6/np.sqrt(math.pi)) * \\\n","                 integrate.quad(lambda y: special.erf(y/np.sqrt(3))*np.exp(-y**2), 0, x/2)[0] \\\n","                 / math.erf(x/2), xl0=0.01)\n","        self.sigma = root(lambda x: self.T3 - (6/np.sqrt(math.pi)) * \\\n","                 integrate.quad(lambda y: special.erf(y/np.sqrt(3))*np.exp(-y**2), 0, x/2)[0] \\\n","                 / math.erf(x/2),\n","                 bracket.bracket[0], bracket.bracket[1])\n","        self.mu = np.log(self.L2 / math.erf(self.sigma/2)) - 0.5*self.sigma**2\n","        self.tau = self.L1 - np.exp(self.mu + 0.5*self.sigma**2)\n","```"],"metadata":{"id":"UtYyxmgZosu-"}},{"cell_type":"markdown","source":["## Fitting the above distributions with MOM assuming the mean is linearly changing with time.\n","\n","First fit a linear trend to the time series of annual maxima, $Q_t$.\n","\n","$$Q_t = \\alpha_0 + \\alpha_1 t$$\n","\n","This is the trend in the mean of the LN distributions:\n","\n","$$E[Q_t] = \\alpha_0 + \\alpha_1 t$$\n","\n","Note, if you were fitting an LP3 distribution, since we estimate that by fitting a P3 distribution to the log of the data, you would fit a linear trend to the log-transformed time series, $X_t=\\ln(Q_t)$."],"metadata":{"id":"Hq3PndAFsMvt"}},{"cell_type":"code","source":["trend_model = smf.ols(formula=\"peak_va ~  Year\", data=flow_df)\n","trend_result = trend_model.fit()\n","print(trend_result.summary())"],"metadata":{"id":"62-kaDjltByw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the fit."],"metadata":{"id":"qFY8paR4vXeZ"}},{"cell_type":"code","source":["flow_df['trend'] = trend_result.predict()\n","plt.plot(flow_df['Year'], flow_df['peak_va'], color=\"tab:blue\")\n","plt.plot(flow_df['Year'], flow_df['trend'], color=\"tab:red\")\n","plt.show()"],"metadata":{"id":"7AQ7UH_pvY64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the diagnostics of this fit."],"metadata":{"id":"Xbu9tx5QXF7Z"}},{"cell_type":"code","source":["fig ,axes = plt.subplots(2,2)\n","sns.residplot(x=trend_result.fittedvalues, y=trend_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax=axes[0,0])\n","sm.qqplot(trend_result.resid,ss.norm,fit=True,line='45',ax=axes[0,1])\n","sm.graphics.tsa.plot_acf(trend_result.resid, ax=axes[1,0])\n","sm.graphics.tsa.plot_pacf(trend_result.resid, ax=axes[1,1])"],"metadata":{"id":"x2itYfQFXHil"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There does seem to be increasing variance and perhaps non-normality, so let's try fitting the trend to the log of the data.\n","\n","$$\\ln(Q_t) = \\alpha_0 + \\alpha_1 t$$\n","\n","This equates to the following trend for the mean:\n","\n","$$E[Q_t] = \\exp(\\alpha_0 + \\alpha_1 t)$$"],"metadata":{"id":"Cp8O-LkAYEn4"}},{"cell_type":"code","source":["flow_df['log_peak'] = np.log(flow_df['peak_va'])\n","trend_model = smf.ols(formula=\"log_peak ~  Year\", data=flow_df)\n","trend_result = trend_model.fit()\n","print(trend_result.summary())"],"metadata":{"id":"XB_tA8G8Yn0D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the fit."],"metadata":{"id":"o8pAQRR6YnX6"}},{"cell_type":"code","source":["flow_df['log_trend'] = trend_result.predict()\n","plt.plot(flow_df['Year'], flow_df['log_peak'], color=\"tab:blue\")\n","plt.plot(flow_df['Year'], flow_df['log_trend'], color=\"tab:red\")\n","plt.show()"],"metadata":{"id":"d5TQA96PY7ts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the diagnostics of this fit."],"metadata":{"id":"KGShTvUPZFNg"}},{"cell_type":"code","source":["fig ,axes = plt.subplots(2,2)\n","sns.residplot(x=trend_result.fittedvalues, y=trend_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax=axes[0,0])\n","sm.qqplot(trend_result.resid,ss.norm,fit=True,line='45',ax=axes[0,1])\n","sm.graphics.tsa.plot_acf(trend_result.resid, ax=axes[1,0])\n","sm.graphics.tsa.plot_pacf(trend_result.resid, ax=axes[1,1])"],"metadata":{"id":"kZiRjiEkZGj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks a lot better! So we'll fit a non-stationary LN2 and LN3 distribution with MOM assuming the log-space mean is changing linearly.\n","\n","To do this, we need to add a function for non-stationary fitting of the LogNormal distribution, $\\texttt{NSfit}$. For now, we'll just include MOM. It will have all the same code as the stationary fit, but we won't be first estimating the moments from the data - we'll run a loop to estimate them at each time step from the linear trend(s)."],"metadata":{"id":"SZ0ULOE_5Sd5"}},{"cell_type":"code","source":["import math\n","from scipy.optimize import brentq as root\n","import scipy.integrate as integrate\n","import scipy.special as special\n","\n","class LogNormal(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","    self.sigma = None\n","    self.tau = None\n","\n","  def fit(self, data, method, npars, initialize=True):\n","    assert method == 'MLE' or method == 'MOM' or method == \"Lmom\",\"method must = 'MLE','MOM', or 'Lmom'\"\n","    assert npars == 2 or npars == 3,\"npars must = 2 or 3\"\n","\n","    self.findMoments(data)\n","    self.findLmoments(data)\n","    if method == 'MLE':\n","      if initialize == False:\n","        if npars == 2:\n","          shape, loc, scale = ss.lognorm.fit(data, floc=0)\n","        elif npars == 3:\n","          shape, loc, scale = ss.lognorm.fit(data)\n","      else:\n","        if npars == 2:\n","          self.fit(data, 'Lmom', 2)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma, floc=0)\n","        elif npars == 3:\n","          self.fit(data, 'Lmom', 3)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma)\n","\n","      self.mu = np.log(scale)\n","      self.sigma = shape\n","      self.tau = loc\n","    elif method == 'MOM':\n","      if npars == 2:\n","        self.sigma = np.sqrt(np.log(1+self.var/self.xbar**2))\n","        self.mu = np.log(self.xbar) - 0.5*self.sigma**2\n","        self.tau = 0\n","      elif npars == 3:\n","        self.sigma = root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                   0.01, np.std(np.log(data),ddof=1))\n","        self.mu = 0.5 * (np.log(self.var / (np.exp(self.sigma**2)-1)) - self.sigma**2)\n","        self.tau = self.xbar - np.exp(self.mu + 0.5*self.sigma**2)\n","    elif method == 'Lmom':\n","      if npars == 2:\n","        self.tau = 0\n","        self.sigma = root(lambda x: special.erf(x/2) - self.L2/self.L1, 0.01, self.L2)\n","        self.mu = np.log(self.L1) - 0.5*self.sigma**2\n","      elif npars == 3:\n","        self.sigma = root(lambda x: self.T3 - (6/np.sqrt(math.pi)) * \\\n","                 integrate.quad(lambda y: special.erf(y/np.sqrt(3))*np.exp(-y**2), 0, x/2)[0] \\\n","                 / math.erf(x/2),\n","                 0.01, 5*self.L2)\n","        self.mu = np.log(self.L2 / math.erf(self.sigma/2)) - 0.5*self.sigma**2\n","        self.tau = self.L1 - np.exp(self.mu + 0.5*self.sigma**2)\n","\n","  def NSfit(self, data, method, npars):\n","    if method == 'MOM':\n","      if npars == 2:\n","        self.sigma = np.sqrt(np.log(1+self.var/self.xbar**2))\n","        self.mu = np.log(self.xbar) - 0.5*self.sigma**2\n","        self.tau = 0\n","      elif npars == 3:\n","        self.sigma = root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                  0.01, 20)\n","        self.mu = 0.5 * (np.log(self.var / (np.exp(self.sigma**2)-1)) - self.sigma**2)\n","        self.tau = self.xbar - np.exp(self.mu + 0.5*self.sigma**2)\n","\n","  def findReturnPd(self, T):\n","    q_T = ss.lognorm.ppf(1-1/T, self.sigma, self.tau, np.exp(self.mu))\n","    return q_T"],"metadata":{"id":"D3VvdzfD2znI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's loop through every time step and re-fit the distribution parameters for the changing mean and estimate the 100-yr flood for each of those time steps based on those parameters."],"metadata":{"id":"sf1zPZ4m5oFW"}},{"cell_type":"code","source":["for npar in npars:\n","  mus = np.empty(len(flow_df[\"Year\"]))\n","  sigmas = np.empty(len(flow_df[\"Year\"]))\n","  taus = np.empty(len(flow_df[\"Year\"]))\n","  q100s = np.empty(len(flow_df[\"Year\"]))\n","  for i, year in enumerate(flow_df[\"Year\"]):\n","    distfit = LogNormal()\n","    # mean estimated from the trend in this year\n","    distfit.xbar = np.exp(flow_df[\"log_trend\"].iloc[i])\n","\n","    # variance and skewness from all the data\n","    distfit.var = np.var(flow_df[\"peak_va\"],ddof=1)\n","    distfit.skew = ss.skew(flow_df[\"peak_va\"],bias=False)\n","\n","    # fit the distribution using this year's mean\n","    distfit.NSfit(flow_df[\"Year\"], 'MOM', npar)\n","\n","    # record this year's parameter estimates\n","    mus[i] = distfit.mu\n","    sigmas[i] = distfit.sigma\n","    taus[i] = distfit.tau\n","\n","    # estimated 100-yr flood in this year\n","    q100s[i] = distfit.findReturnPd(100)\n","\n","  # estimate the return period of the largest flood in the year it happened\n","  # based on the non-stationary fit\n","  index = np.argmax(flow_df[\"peak_va\"])\n","  Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), sigmas[index], taus[index], np.exp(mus[index])))\n","  print(\"Return period of largest flood under non-stationary LN%d MOM fit: %0.0f year\" % (npar, Tmax))\n","\n","  l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","  l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","  plt.title(\"Non-stationary LN\" + str(npar) + \" MOM Fit\")\n","  plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","  plt.show()"],"metadata":{"id":"BDI43RqCtCHu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting the above distributions with MOM assuming the log-space variance are linearly changing with time."],"metadata":{"id":"RvySaYsksWqd"}},{"cell_type":"markdown","source":["The variance has to be positive. To ensure this, a common assumption is that its log is changing linearly with time too. Then the following equation can be fit:\n","\n","$$\\ln[{(Q_t - E[Q_t])^2}] = \\beta_0 + \\beta_1 t$$\n","\n","where $Q_t$ are the annual maxima and $E[Q_t]$ is estimated from the previous regression equation. This equates to the following relationships for the variance and standard deviation:\n","\n","$$\\text{Var}(Q_t) = \\exp(\\beta_0 + \\beta_1 t)$$\n","$$\\text{Std}(Q_t) = \\sqrt{\\exp(\\beta_0 + \\beta_1 t)}$$\n","\n","Again, note if fitting a P3 distribution to the log of the data for a non-stationary LP3 distribution, you would fit the regression to the log-space variance $\\ln[{(X_t - E[X_t])^2}]$ where $X_t=\\ln(Q_t)$ and $E[X_t]$ is its mean."],"metadata":{"id":"creYX-Hf8GpB"}},{"cell_type":"code","source":["flow_df[\"log_var\"] = np.log((flow_df[\"peak_va\"] - flow_df[\"trend\"])**2)\n","\n","trend2_model = smf.ols(formula=\"log_var ~ Year\", data=flow_df)\n","trend2_result = trend2_model.fit()\n","print(trend2_result.summary())"],"metadata":{"id":"l2syTpGRtCjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the fit."],"metadata":{"id":"7r90_kW1B1bz"}},{"cell_type":"code","source":["flow_df['logvar_trend'] = trend2_result.predict()\n","plt.plot(flow_df['Year'], flow_df['log_var'], color=\"tab:blue\")\n","plt.plot(flow_df['Year'], flow_df['logvar_trend'], color=\"tab:red\")\n","plt.show()"],"metadata":{"id":"lumO4nUvB2K6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the diagnostics."],"metadata":{"id":"i5BmtBXCZmyu"}},{"cell_type":"code","source":["fig, axes = plt.subplots(2,2)\n","sns.residplot(x=trend2_result.fittedvalues, y=trend2_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax=axes[0,0])\n","sm.qqplot(trend2_result.resid,ss.norm,fit=True,line='45',ax=axes[0,1])\n","sm.graphics.tsa.plot_acf(trend2_result.resid, ax=axes[1,0])\n","sm.graphics.tsa.plot_pacf(trend2_result.resid, ax=axes[1,1])"],"metadata":{"id":"l1JqOL_2ZorX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The variance of the residuals is relatively constant, and they are independent, but they are not normal. **For this reason, we should probably not proceed with this fit in practice, but find another transformation**, but we will illustrate the process below as an example.\n","\n","To do this, we'll loop through every time step and re-fit the distribution parameters for the changing mean **and variance** and estimate the 100-yr flood for each of those time steps based on those parameters."],"metadata":{"id":"8q5x6D5pCFgM"}},{"cell_type":"code","source":["for npar in npars:\n","  mus = np.empty(len(flow_df[\"Year\"]))\n","  sigmas = np.empty(len(flow_df[\"Year\"]))\n","  taus = np.empty(len(flow_df[\"Year\"]))\n","  q100s = np.empty(len(flow_df[\"Year\"]))\n","  for i, year in enumerate(flow_df[\"Year\"]):\n","    distfit = LogNormal()\n","    # mean and variance estimated from the trend in this year\n","    distfit.xbar = np.exp(flow_df[\"log_trend\"].iloc[i])\n","    distfit.var = np.exp(flow_df[\"logvar_trend\"].iloc[i])\n","\n","    # skewness from all the data\n","    distfit.skew = ss.skew(flow_df[\"peak_va\"],bias=False)\n","\n","    # fit the distribution using this year's mean and variance\n","    distfit.NSfit(flow_df[\"Year\"], 'MOM', npar)\n","\n","    # record this year's parameter estimates\n","    mus[i] = distfit.mu\n","    sigmas[i] = distfit.sigma\n","    taus[i] = distfit.tau\n","\n","    # estimated 100-yr flood in this year\n","    q100s[i] = distfit.findReturnPd(100)\n","\n","  # estimate the return period of the largest flood in the year it happened\n","  # based on the non-stationary fit\n","  index = np.argmax(flow_df[\"peak_va\"])\n","  Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), sigmas[index], taus[index], np.exp(mus[index])))\n","  print(\"Return period of largest flood under non-stationary LN%d MOM fit: %0.0f year\" % (npar, Tmax))\n","\n","  l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","  l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","  plt.title(\"Non-stationary LN\" + str(npar) + \" MOM Fit\")\n","  plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","  plt.show()"],"metadata":{"id":"Hn8IEhoyB-Gf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These fits don't seem reasonable, as the 100-yr flood estimate is exceeded several times in 89 years. This is perhaps not surprising given the issues with the diagnostics of the fitted trend in the log of the variance."],"metadata":{"id":"xGyzs7fZTHcf"}},{"cell_type":"markdown","source":["## Fitting LN3 distributions with MOM assuming the mean, variance, and skewness are changing linearly with time.\n","\n","If the coefficient of skewness is changing linearly with time too, the following equation can be fit:\n","\n","$$[(Q_t - E[Q_t])/\\text{Std}(Q_t)]^3 = \\gamma_0 + \\gamma_1 t$$\n","\n","where $Q_t$ are the annual maxima and $E[Q_t]$ and $\\text{Std}(Q_t)$ are estimated from the previous regression equations.\n","\n","Again, note if fitting a P3 distribution to the log of the data for a non-stationary LP3 distribution, you would fit the above regression to the log-space variance $[(X_t - E[X_t])/\\text{Std}(X_t)]^3$ where $X_t=\\ln(Q_t)$ and $E[X_t]$ and $\\text{Std}(X_t)$ are its mean and standard deviation."],"metadata":{"id":"EoaCoMWosbqS"}},{"cell_type":"code","source":["flow_df[\"std_trend\"] = np.sqrt(np.exp(flow_df[\"logvar_trend\"]))\n","flow_df[\"skew\"] = ((flow_df[\"peak_va\"] - flow_df[\"trend\"])/flow_df[\"std_trend\"])**3\n","\n","trend3_model = smf.ols(formula=\"skew ~ Year\", data=flow_df)\n","trend3_result = trend3_model.fit()\n","print(trend3_result.summary())"],"metadata":{"id":"PfxXtSxbzPlq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note, this is not statistically significant, so we probably should not do this**, but we'll show the approach as an example."],"metadata":{"id":"SgFThkXIKIXv"}},{"cell_type":"code","source":["flow_df['skew_trend'] = trend3_result.predict()\n","plt.plot(flow_df['Year'], flow_df['skew'], color=\"tab:blue\")\n","plt.plot(flow_df['Year'], flow_df['skew_trend'], color=\"tab:red\")\n","plt.show()"],"metadata":{"id":"UjIwrnIvKO61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the diagnostics."],"metadata":{"id":"5E5i9bulatHh"}},{"cell_type":"code","source":["fig, axes = plt.subplots(2,2)\n","sns.residplot(x=trend3_result.fittedvalues, y=trend3_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax=axes[0,0])\n","sm.qqplot(trend2_result.resid,ss.norm,fit=True,line='45',ax=axes[0,1])\n","sm.graphics.tsa.plot_acf(trend3_result.resid, ax=axes[1,0])\n","sm.graphics.tsa.plot_pacf(trend3_result.resid, ax=axes[1,1])"],"metadata":{"id":"tExe1EvNauW0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not surprisingly, **the diagnostics on this trend are also poor, further reinforcing that we should not do this!** But we show how to as an example below.\n","\n","To do this, we'll loop through every time step and re-fit the distribution parameters for the changing mean, variance, **and skew** and estimate the 100-yr flood for each of those time steps based on those parameters."],"metadata":{"id":"AT-eERzAN2mx"}},{"cell_type":"code","source":["for npar in npars:\n","  mus = np.empty(len(flow_df[\"Year\"]))\n","  sigmas = np.empty(len(flow_df[\"Year\"]))\n","  taus = np.empty(len(flow_df[\"Year\"]))\n","  q100s = np.empty(len(flow_df[\"Year\"]))\n","  for i, year in enumerate(flow_df[\"Year\"]):\n","    distfit = LogNormal()\n","    # mean, variance and skewness estimated from the trend in this year\n","    distfit.xbar = np.exp(flow_df[\"log_trend\"].iloc[i])\n","    distfit.var = np.exp(flow_df[\"logvar_trend\"].iloc[i])\n","    distfit.skew = flow_df[\"skew_trend\"].iloc[i]\n","\n","    # fit the distribution using this year's mean and variance\n","    distfit.NSfit(flow_df[\"Year\"], 'MOM', npar)\n","\n","    # record this year's parameter estimates\n","    mus[i] = distfit.mu\n","    sigmas[i] = distfit.sigma\n","    taus[i] = distfit.tau\n","\n","    # estimated 100-yr flood in this year\n","    q100s[i] = distfit.findReturnPd(100)\n","\n","  # estimate the return period of the largest flood in the year it happened\n","  # based on the non-stationary fit\n","  index = np.argmax(flow_df[\"peak_va\"])\n","  Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), sigmas[index], taus[index], np.exp(mus[index])))\n","  print(\"Return period of largest flood under non-stationary LN%d MOM fit: %0.0f year\" % (npar, Tmax))\n","\n","  l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","  l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","  plt.title(\"Non-stationary LN\" + str(npar) + \" MOM Fit\")\n","  plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","  plt.show()"],"metadata":{"id":"c5NKg7jVNzOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The LN2 fit again seems unreasonable given the frequent exceedance of the 100-yr flood estimate over the 89-year record.\n","\n","The LN3 estimation gets a sign error with the root function, indicating we would have to widen our bounds, although we then risk an overflow in the exponential. Given the poor fits in the trends, this does not seem worth playing around with."],"metadata":{"id":"BHLrWe4UUuVl"}},{"cell_type":"markdown","source":["## Fitting the above distributions with MLE assuming non-stationarity in 1, 2, or 3 of the parameters.\n","\n","To do this, we need to write the log-likelihood function as the sum of the log(pdf) evaluated at data point $i$ using the parameter values at time step $i$. This will depend on how many parameters are assumed to be linearly changing with time, so we need separate cases defining the log-likelihood depending on how many parameters are non-stationary.\n","\n","Then we estimate those parameters using optimization to find which values maximize the log-likelihood. We can use [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), which requires to inputs: a function that takes in decision variables (x) and returns an objective to be minimizes, and initial estimates of x. We'll return the negated log-likelihood since minimizing that is equivalent to maximizing the likelihood. We'll initialize the parameter estimates at their stationary values (so assuming 0 slope and an intercept at the stationary value)."],"metadata":{"id":"yrWADtgusuEG"}},{"cell_type":"code","source":["from scipy.optimize import minimize\n","\n","class LogNormal(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","    self.sigma = None\n","    self.tau = None\n","\n","  def fit(self, data, method, npars, initialize=True):\n","    assert method == 'MLE' or method == 'MOM' or method == \"Lmom\",\"method must = 'MLE','MOM', or 'Lmom'\"\n","    assert npars == 2 or npars == 3,\"npars must = 2 or 3\"\n","\n","    self.findMoments(data)\n","    self.findLmoments(data)\n","    if method == 'MLE':\n","      if initialize == False:\n","        if npars == 2:\n","          shape, loc, scale = ss.lognorm.fit(data, floc=0)\n","        elif npars == 3:\n","          shape, loc, scale = ss.lognorm.fit(data)\n","      else:\n","        if npars == 2:\n","          self.fit(data, 'Lmom', 2)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma, floc=0)\n","        elif npars == 3:\n","          self.fit(data, 'Lmom', 3)\n","          shape, loc, scale = ss.lognorm.fit(data, self.sigma)\n","\n","      self.mu = np.log(scale)\n","      self.sigma = shape\n","      self.tau = loc\n","    elif method == 'MOM':\n","      if npars == 2:\n","        self.sigma = np.sqrt(np.log(1+self.var/self.xbar**2))\n","        self.mu = np.log(self.xbar) - 0.5*self.sigma**2\n","        self.tau = 0\n","      elif npars == 3:\n","        self.sigma = root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                   0.01, np.std(np.log(data),ddof=1))\n","        self.mu = 0.5 * (np.log(self.var / (np.exp(self.sigma**2)-1)) - self.sigma**2)\n","        self.tau = self.xbar - np.exp(self.mu + 0.5*self.sigma**2)\n","    elif method == 'Lmom':\n","      if npars == 2:\n","        self.tau = 0\n","        self.sigma = root(lambda x: special.erf(x/2) - self.L2/self.L1, 0.01, self.L2)\n","        self.mu = np.log(self.L1) - 0.5*self.sigma**2\n","      elif npars == 3:\n","        self.sigma = root(lambda x: self.T3 - (6/np.sqrt(math.pi)) * \\\n","                 integrate.quad(lambda y: special.erf(y/np.sqrt(3))*np.exp(-y**2), 0, x/2)[0] \\\n","                 / math.erf(x/2),\n","                 0.01, 5*self.L2)\n","        self.mu = np.log(self.L2 / math.erf(self.sigma/2)) - 0.5*self.sigma**2\n","        self.tau = self.L1 - np.exp(self.mu + 0.5*self.sigma**2)\n","\n","  def NSfit(self, data, method, npars, nNSpars):\n","    if method == 'MOM':\n","      if npars == 2:\n","        self.sigma = np.sqrt(np.log(1+self.var/self.xbar**2))\n","        self.mu = np.log(self.xbar) - 0.5*self.sigma**2\n","        self.tau = 0\n","      elif npars == 3:\n","        self.sigma = root(lambda x: (np.exp(3*x**2)-3*np.exp(x**2)+2) / (np.exp(x**2)-1)**(3/2) - self.skew,\n","                  0.01, 20)\n","        self.mu = 0.5 * (np.log(self.var / (np.exp(self.sigma**2)-1)) - self.sigma**2)\n","        self.tau = self.xbar - np.exp(self.mu + 0.5*self.sigma**2)\n","    elif method == \"MLE\":\n","      if npars == 2:\n","        # get initial parameter estimates\n","        shape, loc, scale = ss.lognorm.fit(data, floc=0)\n","        if nNSpars == 1:\n","          x0 = [shape, scale, 0]\n","          def logLik(x):\n","            logLik = 0\n","            for i in range(len(data)):\n","              logLik += ss.lognorm.logpdf(data[i], x[0], 0, x[1] + x[2]*i)\n","            return -logLik\n","          result = minimize(logLik, x0)\n","          self.sigma = result.x[0]\n","          self.tau = 0\n","          self.mu = np.log(result.x[1] + result.x[2]*np.arange(len(data)))\n","        elif nNSpars == 2:\n","          x0 = [shape, 0, scale, 0]\n","          def logLik(x):\n","            logLik = 0\n","            for i in range(len(data)):\n","              logLik += ss.lognorm.logpdf(data[i], x[0] + x[1]*i, 0, x[2] + x[3]*i)\n","            return -logLik\n","          # add constraints that shape parameter has to be positive at every time step\n","          constraints = []\n","          for i in range(len(data)):\n","            constraints.append({'type': 'ineq', 'fun': lambda x: x[0] + x[1]*i})\n","          result = minimize(logLik, x0, constraints=constraints)\n","          self.sigma = result.x[0] + result.x[1]*np.arange(len(data))\n","          self.tau = 0\n","          self.mu = np.log(result.x[2] + result.x[3]*np.arange(len(data)))\n","      elif npars == 3:\n","        # get initial parameter estimates\n","        shape, loc, scale = ss.lognorm.fit(data)\n","        if nNSpars == 1:\n","          x0 = [shape, loc, scale, 0]\n","          def logLik(x):\n","            logLik = 0\n","            for i in range(len(data)):\n","              logLik += ss.lognorm.logpdf(data[i], x[0], x[1], x[2] + x[3]*i)\n","            return -logLik\n","          result = minimize(logLik, x0)\n","          self.sigma = result.x[0]\n","          self.tau = result.x[1]\n","          self.mu = np.log(result.x[2] + result.x[3]*np.arange(len(data)))\n","        elif nNSpars == 2:\n","          x0 = [shape, 0, loc, scale, 0]\n","          def logLik(x):\n","            logLik = 0\n","            for i in range(len(data)):\n","              logLik += ss.lognorm.logpdf(data[i], x[0] + x[1]*i, x[2], x[3] + x[4]*i)\n","            return -logLik\n","          # add constraints that shape parameter has to be positive at every time step\n","          constraints = []\n","          for i in range(len(data)):\n","            constraints.append({'type': 'ineq', 'fun': lambda x: x[0] + x[1]*i})\n","          result = minimize(logLik, x0, constraints=constraints)\n","          self.sigma = result.x[0] + result.x[1]*np.arange(len(data))\n","          self.tau = result.x[2]\n","          self.mu = np.log(result.x[3] + result.x[4]*np.arange(len(data)))\n","        elif nNSpars == 3:\n","          x0 = [shape, 0, loc, 0, scale, 0]\n","          def logLik(x):\n","            logLik = 0\n","            for i in range(len(data)):\n","              logLik += ss.lognorm.logpdf(data[i], x[0] + x[1]*i, x[2] + x[3]*i, x[4] + x[5]*i)\n","            return -logLik\n","          # add constraints that shape parameter has to be positive at every time step\n","          constraints = []\n","          for i in range(len(data)):\n","            constraints.append({'type': 'ineq', 'fun': lambda x: x[0] + x[1]*i})\n","          result = minimize(logLik, x0, constraints=constraints)\n","          self.sigma = result.x[0] + result.x[1]*np.arange(len(data))\n","          self.tau = result.x[2] + result.x[3]*np.arange(len(data))\n","          self.mu = np.log(result.x[4] + result.x[5]*np.arange(len(data)))\n","\n","  def findReturnPd(self, T):\n","    q_T = ss.lognorm.ppf(1-1/T, self.sigma, self.tau, np.exp(self.mu))\n","    return q_T"],"metadata":{"id":"4Mv8YkY0zb1A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## First let's assume only the scale parameter is linearly changing with time."],"metadata":{"id":"mHw613wLgJrD"}},{"cell_type":"code","source":["for npar in npars:\n","  distfit = LogNormal()\n","  distfit.NSfit(np.array(flow_df[\"peak_va\"]), \"MLE\", npar, 1)\n","  mus = distfit.mu\n","\n","  # estimate 100-yr flood each year\n","  q100s = np.empty(len(flow_df[\"Year\"]))\n","  for i, year in enumerate(flow_df[\"Year\"]):\n","    distfit.mu = mus[i]\n","    q100s[i] = distfit.findReturnPd(100)\n","\n","  # estimate the return period of the largest flood in the year it happened\n","  # based on the non-stationary fit\n","  index = np.argmax(flow_df[\"peak_va\"])\n","  Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), distfit.sigma, distfit.tau, np.exp(mus[index])))\n","  print(\"Return period of largest flood under non-stationary LN%d MLE fit: %0.0f year\" % (npar, Tmax))\n","\n","  l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","  l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","  plt.title(\"Non-stationary LN\" + str(npar) + \" MLE Fit\")\n","  plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","  plt.show()"],"metadata":{"id":"Cg2l8uhLg4nG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Now assume the scale and shape parameters are linearly changing with time."],"metadata":{"id":"Wg5v-ArzsoEd"}},{"cell_type":"code","source":["for npar in npars:\n","  distfit = LogNormal()\n","  distfit.NSfit(np.array(flow_df[\"peak_va\"]), \"MLE\", npar, 2)\n","  mus = distfit.mu\n","  sigmas = distfit.sigma\n","\n","  # estimate 100-yr flood each year\n","  q100s = np.empty(len(flow_df[\"Year\"]))\n","  for i, year in enumerate(flow_df[\"Year\"]):\n","    distfit.mu = mus[i]\n","    distfit.sigma = sigmas[i]\n","    q100s[i] = distfit.findReturnPd(100)\n","\n","  # estimate the return period of the largest flood in the year it happened\n","  # based on the non-stationary fit\n","  index = np.argmax(flow_df[\"peak_va\"])\n","  Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), distfit.sigma, distfit.tau, np.exp(mus[index])))\n","  print(\"Return period of largest flood under non-stationary LN%d MLE fit: %0.0f year\" % (npar, Tmax))\n","\n","  l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","  l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","  plt.title(\"Non-stationary LN\" + str(npar) + \" MLE Fit\")\n","  plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","  plt.show()"],"metadata":{"id":"17doEJkczcIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finally, assume the scale, shape, and location parameters are changing linearly with time."],"metadata":{"id":"GkkQkkQSsy-H"}},{"cell_type":"code","source":["distfit = LogNormal()\n","distfit.NSfit(np.array(flow_df[\"peak_va\"]), \"MLE\", 3, 3)\n","mus = distfit.mu\n","sigmas = distfit.sigma\n","taus = distfit.tau\n","\n","# estimate 100-yr flood each year\n","q100s = np.empty(len(flow_df[\"Year\"]))\n","for i, year in enumerate(flow_df[\"Year\"]):\n","  distfit.mu = mus[i]\n","  distfit.sigma = sigmas[i]\n","  distfit.tau = taus[i]\n","  q100s[i] = distfit.findReturnPd(100)\n","\n","# estimate the return period of the largest flood in the year it happened\n","# based on the non-stationary fit\n","index = np.argmax(flow_df[\"peak_va\"])\n","Tmax = 1/(1 - ss.lognorm.cdf(np.max(flow_df[\"peak_va\"]), distfit.sigma, distfit.tau, np.exp(mus[index])))\n","print(\"Return period of largest flood under non-stationary LN%d MLE fit: %0.0f year\" % (npar, Tmax))\n","\n","l1, = plt.plot(flow_df[\"Year\"], flow_df[\"peak_va\"], color='tab:blue')\n","l2, = plt.plot(flow_df[\"Year\"], q100s, color=\"tab:red\")\n","plt.title(\"Non-stationary LN\" + str(npar) + \" MLE Fit\")\n","plt.legend([l1,l2],['Annual Maxima','100-yr Flood'])\n","plt.show()"],"metadata":{"id":"ZYikr5AKzqa3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Across all these approaches, we've gotten vastly different 100-yr flood estimates! This illustrates the greater uncertainty associated with non-stationary flood frequency analysis, highlighting the tradeoff between bias and variance."],"metadata":{"id":"MIXftP5TsM0F"}}],"metadata":{"colab":{"provenance":[{"file_id":"1UmoqDi9mnlLQlre5qrqmVZaQON6f63oF","timestamp":1741794802118}],"authorship_tag":"ABX9TyOmKpWhtNWLO7GxbOochzWV"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}