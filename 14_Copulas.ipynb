{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcLcvNablYOkwdJpFF2v8F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Characterizing Drought Duration and Severity with Copulas\n","\n","We'll use paleo-reconstructed Palmer Drought Sseverity Index (PDSI) estimates from https://www.treeflow.info/sites/default/files/east_colo_pdsi.txt to build a model of the relationship between drought duration and severity."],"metadata":{"id":"NSu113KEy04Q"}},{"cell_type":"code","source":["!pip install lmoments3"],"metadata":{"id":"04lgWiLH2wj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","\n","# allow access to google drive\n","drive.mount('/content/drive')\n","\n","!cp \"drive/MyDrive/Colab Notebooks/CE6280/CodingExamples/utils.py\" .\n","from utils import *\n","\n","PDSI = pd.read_csv('drive/MyDrive/Colab Notebooks/CE6280/Data/EasternCO_PDSI.csv')"],"metadata":{"id":"4ZM_E3WB2zCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create functions to find drought periods (clusters), and then compute the duration and severity of droughts over those periods."],"metadata":{"id":"pSapg4tFAZ9I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7JQDPkaRyP_s"},"outputs":[],"source":["import numpy as np\n","\n","# find all droughts, defined as consecutive periods below the mean\n","def findClusters(indices):\n","  allClusters = []\n","  subCluster = [indices[0]]\n","  for i in range(1,len(indices)):\n","    if indices[i] - subCluster[-1] <= 1:\n","      subCluster.append(indices[i])\n","    else:\n","      allClusters.append(subCluster)\n","      subCluster = [indices[i]]\n","\n","  allClusters.append(subCluster)\n","\n","  return allClusters\n","\n","def findDandS(droughts, variable):\n","  durations = np.zeros(len(droughts))\n","  severities = np.zeros(len(droughts))\n","  for i,d in enumerate(droughts):\n","    durations[i] = len(d)\n","    severities[i] = -np.sum(variable[d[0]:(d[-1]+1)])\n","\n","  return durations, severities"]},{"cell_type":"markdown","source":["Find drought periods (when PDSI $<$0) and their duration and severity."],"metadata":{"id":"0Vl3XTHVAOo3"}},{"cell_type":"code","source":["drought_indices = np.where(PDSI['reconPDSI'] < 0)[0]\n","droughts = findClusters(drought_indices)\n","\n","durations, severities = findDandS(droughts, PDSI['reconPDSI'])"],"metadata":{"id":"pKZWPF5KAN1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting Marginal Distributions\n","\n","### Drought Severity\n","\n","Visualize the distribution of drought severity."],"metadata":{"id":"rEvmCj1JB956"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.hist(severities)\n","plt.xlabel('Drought Severity')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"4y4KZpefVqzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The drought severities are positively skewed. Let's see if we can fit them with a log-normal distribution."],"metadata":{"id":"tu5MR_0QWo5O"}},{"cell_type":"code","source":["methods = [\"MOM\", \"MLE\", \"Lmom\"]\n","npars = [2, 3]\n","\n","for method in methods:\n","  for npar in npars:\n","    distfit = LogNormal()\n","    distfit.fit(severities, method, npar)\n","    distfit.plotHistPDF(severities, 0, 15, \"LN\" + str(npar) + \" \" + str(method) + \" Fit\")\n","    print(\"LN%d %s mu: %0.2f\" % (npar, method, distfit.mu))\n","    print(\"LN%d %s sigma: %0.2f\" % (npar, method, distfit.sigma))\n","    print(\"LN%d %s tau: %0.2f\" % (npar, method, distfit.tau))\n","    print(\"\\n\")"],"metadata":{"id":"gnGP66YoWwAo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The LN2 distribution fit with MLE visually fits well and has a lower bound of 0, whereas some of the LN3 distributions have negative lower bounds, which is not physically possible. So we will proceed with the LN2 MLE fit."],"metadata":{"id":"tNimyKp-XOOC"}},{"cell_type":"code","source":["distfit = LogNormal()\n","distfit.fit(severities, 'MLE', 2)"],"metadata":{"id":"EqX4oP9JZyz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Drought Duration\n","\n","Visualize the distribution of drought duration."],"metadata":{"id":"G3qr1dL1WTfN"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.hist(durations)\n","plt.xlabel('Drought Duration')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"RJNjOkkHWail"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a positively skewed, discrete distribution. Two possible discrete distributions that could fit this data are the geometric and Poisson distributions. Below we write classes for these distributions and test their goodness of fit with PPCC tests."],"metadata":{"id":"Igoad9COAvMA"}},{"cell_type":"code","source":["from scipy import stats as ss\n","from lmoments3 import distr\n","\n","class Geom(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.p = None\n","\n","  def ppccTest(self, data, title, m=10000):\n","    # calculate test statistic, rho\n","    x_sorted = np.sort(data)\n","    p_observed = ss.mstats.plotting_positions(x_sorted)\n","    x_fitted = ss.geom.ppf(p_observed, self.p)\n","    self.ppcc_rho = np.corrcoef(x_sorted, x_fitted)[0,1]\n","\n","    # generate m synthetic samples of n observations to estimate null distribution of rho\n","    rhoVector = np.zeros(m)\n","    for i in range(m):\n","      np.random.seed(i)\n","      x = ss.geom.rvs(self.p, size=len(data))\n","      rhoVector[i] = np.corrcoef(np.sort(x), x_fitted)[0,1]\n","\n","    # calculate p-value of test and make QQ plot\n","    count = 0\n","    for i in range(len(rhoVector)):\n","      if self.ppcc_rho < rhoVector[i]:\n","        count = count + 1\n","\n","    self.p_value_PPCC = 1 - count/(len(rhoVector) + 1)\n","\n","    # make Q-Q plot\n","    plt.scatter(x_sorted,x_fitted,color='b')\n","    plt.plot(x_sorted,x_sorted,color='r')\n","    plt.xlabel('Observations')\n","    plt.ylabel('Fitted Values')\n","    plt.title(title)\n","    plt.show()\n","\n","class Poisson(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","\n","  def ppccTest(self, data, title, m=10000):\n","    # calculate test statistic, rho\n","    x_sorted = np.sort(data)\n","    p_observed = ss.mstats.plotting_positions(x_sorted)\n","    x_fitted = ss.poisson.ppf(p_observed, self.mu)\n","    self.ppcc_rho = np.corrcoef(x_sorted, x_fitted)[0,1]\n","\n","    # generate m synthetic samples of n observations to estimate null distribution of rho\n","    rhoVector = np.zeros(m)\n","    for i in range(m):\n","      np.random.seed(i)\n","      x = ss.poisson.rvs(self.mu, size=len(data))\n","      rhoVector[i] = np.corrcoef(np.sort(x), x_fitted)[0,1]\n","\n","    # calculate p-value of test and make QQ plot\n","    count = 0\n","    for i in range(len(rhoVector)):\n","      if self.ppcc_rho < rhoVector[i]:\n","        count = count + 1\n","\n","    self.p_value_PPCC = 1 - count/(len(rhoVector) + 1)\n","\n","    # make Q-Q plot\n","    plt.scatter(x_sorted,x_fitted,color='b')\n","    plt.plot(x_sorted,x_sorted,color='r')\n","    plt.xlabel('Observations')\n","    plt.ylabel('Fitted Values')\n","    plt.title(title)\n","    plt.show()"],"metadata":{"id":"uvGyJH4eArUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see if the Poisson distribution captures the drought durations. The Poisson distribution has one parameter $\\mu$ that is estimated as the mean for both MOM and MLE:\n","\n","$f_X(k) = P(X=k) = \\exp(-\\mu) \\frac{\\mu^k}{k!}$  \n","$\\mu = E[X] \\approx \\bar{x}$."],"metadata":{"id":"DG3cGlGAVcBh"}},{"cell_type":"code","source":["poiss_fit = Poisson()\n","poiss_fit.mu = np.mean(durations)\n","poiss_fit.ppccTest(durations, 'Poisson PPCC Test')\n","print('Poisson p-value:', poiss_fit.p_value_PPCC)"],"metadata":{"id":"ddTAqdodBxZy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We strongly reject this fit! How about a geometric distribution? This distribution has one parameter $p$, which is estimated as the inverse of the mean using both MOM and MLE:\n","\n","$f_x(k) = P(X=k) = (1-p)^{k-1}p$  \n","$p = \\frac{1}{E[X]} \\approx \\frac{1}{\\bar{x}}$."],"metadata":{"id":"QlNxICwpB9gy"}},{"cell_type":"code","source":["geom_fit = Geom()\n","geom_fit.p = 1/np.mean(durations)\n","geom_fit.ppccTest(durations, 'Poisson PPCC Test')\n","print('Poisson p-value:', geom_fit.p_value_PPCC)"],"metadata":{"id":"9M4jZTo0CHxC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We do not reject this fit at the 5\\% level, but we do at the 10\\% level, so it is not a very strong fit. For this reason, we'll just use the empirical distribution. This has the advantage of fitting the data perfectly, but the disadvantage of not enabling durations beyond what has been observed in the historical record.\n","\n","## Converting Real-Space Values to \"U-Space\" values through the Marginal CDFs."],"metadata":{"id":"aDage20zCO4D"}},{"cell_type":"code","source":["u_S = ss.lognorm.cdf(severities, distfit.sigma, distfit.tau, np.exp(distfit.mu))\n","u_D = ss.mstats.plotting_positions(durations)\n","\n","rho = np.corrcoef(u_D, u_S)[0,1]\n","print(\"Correlation in U-space: %0.2f\" % rho)\n","\n","plt.scatter(u_D, u_S)\n","plt.xlabel('U-space Drought Duration')\n","plt.ylabel('U-space Drought Severity')\n","plt.show()"],"metadata":{"id":"LxY6RB5tCpek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The overall correlation coefficient is moderate at 0.48. However, the correlation seems weak for short durations, but high for long durations, showing upper tail dependence. The Gumbel and Joe copulas exhibit upper tail dependence, while the Clayton copula exhibits lower tail dependence. Therefore, we could try directly fitting Gumbel and Joe copulas, and we could also fit a Clayton copula to 1-u, as that would reverse the tail dependence (see figure below)."],"metadata":{"id":"kjZNC3bLaxs2"}},{"cell_type":"code","source":["plt.scatter(1-u_D, 1-u_S)\n","plt.xlabel('1 - U-space Drought Duration')\n","plt.ylabel('1 - U-space Drought Severity')\n","plt.show()"],"metadata":{"id":"mwRZmT2FbUaO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting Bivariate Copulas to U-Space Variables"],"metadata":{"id":"vR_eq9M_bdPG"}},{"cell_type":"code","source":["!pip install pyvinecopulib"],"metadata":{"id":"rtlRfozkd5NP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyvinecopulib as pv\n","\n","u = np.vstack((u_D, u_S)).T\n","\n","GumbelCop = pv.Bicop(pv.BicopFamily.gumbel)\n","GumbelCop.fit(data=u)\n","print(GumbelCop)\n","print(\"AIC: %0.2f\" % GumbelCop.aic())\n","print(\"BIC: %0.2f\" % GumbelCop.bic())\n","print(\"log-likelihood: %0.2f\" % GumbelCop.loglik())"],"metadata":{"id":"Z2C70wXIbv6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["JoeCop = pv.Bicop(pv.BicopFamily.joe)\n","JoeCop.fit(data=u)\n","print(JoeCop)\n","print(\"AIC: %0.2f\" % JoeCop.aic())\n","print(\"BIC: %0.2f\" % JoeCop.bic())\n","print(\"log-likelihood: %0.2f\" % JoeCop.loglik())"],"metadata":{"id":"Fnu_QeOHfaPE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ClaytonCop = pv.Bicop(pv.BicopFamily.clayton)\n","ClaytonCop.fit(data=1-u)\n","print(ClaytonCop)\n","print(\"AIC: %0.2f\" % ClaytonCop.aic())\n","print(\"BIC: %0.2f\" % ClaytonCop.bic())\n","print(\"log-likelihood: %0.2f\" % ClaytonCop.loglik())"],"metadata":{"id":"14t9lFjifd60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["controls = pv.FitControlsBicop(parametric_method=\"mle\", selection_criterion=\"bic\")\n","\n","BestCop = pv.Bicop.from_data(data=u, controls=controls)\n","print(BestCop)\n","\n","print(\"AIC: %0.2f\" % BestCop.aic())\n","print(\"BIC: %0.2f\" % BestCop.bic())\n","print(\"log-likelihood: %0.2f\" % BestCop.loglik())"],"metadata":{"id":"8987HMCshBD4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing Goodness of Fit with Cramer von Mises test\n","\n","The Cramer von Mises test statistic is:\n","\n","$\\omega^2 = \\sum_{i=1}^n [S_n(x_i) - S_n^*(x_i)]^2$  \n","where  \n","$S_n(x_i)$ and $S_n^*(x_i)$ are the empirical and fitted copulas:  \n","$S_n(x_i) = \\frac{1}{n+1} \\sum_{i=1}^n \\mathbb{1} \\{P(C(u_i)<x_i)\\}$ and  \n","$S_n^*(x_i) = P(C(u)<x_i)$\n","\n","We can use a Monte Carlo simulation from the fitted copula to compute the null distribution of $\\omega^2$. If our test statistic is above more than 95\\% of them, we reject that the data came from the fitted copula at a significance level of 0.05. The p-value is the percent of simulated test statistics that are greater than the observed test statistic.\n","\n","The functions below computes the test statistic and p-value of the Cramer von Mises test."],"metadata":{"id":"P16VEfmuM_Ue"}},{"cell_type":"code","source":["def compute_omegaSq(data, cop):\n","  n = len(data)\n","  omegaSq = 0\n","  for i in range(n):\n","    count = len(np.where(np.all(data <= data[i,:]))[0])\n","    omegaSq += (count/(n+1) - cop.cdf(np.array([data[i,:]]))[0])**2\n","\n","  return omegaSq\n","\n","def cramer_von_mises(data, cop, m=1000):\n","  # compute test statistic, omegaSq\n","  omegaSq = compute_omegaSq(data, cop)\n","\n","  # generate m synthetic samples of n observations to estimate null distribution of omegaSq\n","  omegaSqVector = np.zeros(m)\n","  for i in range(m):\n","    uSim = cop.simulate(len(data), qrng=True, seeds=[i])\n","    omegaSqVector[i] = compute_omegaSq(uSim, cop)\n","\n","  # calculate p-value of test\n","  p_value = len(np.where(omegaSqVector > omegaSq)[0]) / (len(omegaSqVector) + 1)\n","\n","  return p_value"],"metadata":{"id":"uuqhiy-sNCT9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's compute the p-value for each fitted distribution."],"metadata":{"id":"zkvv7bpzWKPp"}},{"cell_type":"code","source":["p_Gumbel = cramer_von_mises(u, GumbelCop)\n","p_Joe = cramer_von_mises(u, JoeCop)\n","p_Clayton = cramer_von_mises(1-u, ClaytonCop)\n","\n","print(\"Gumbel p-value: %0.2f\" % p_Gumbel)\n","print(\"Joe p-value: %0.2f\" % p_Joe)\n","print(\"Clayton p-value: %0.2f\" % p_Clayton)"],"metadata":{"id":"mWcC0TAhWSzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This suggests the Clayton copula is the best, which differs from the other metrics. We'll show fits from both."],"metadata":{"id":"qrB0iM5K0eRL"}},{"cell_type":"markdown","source":["## Plotting Fitted Bivariate Copulas\n","\n","3D PDF. Keep in mind the quantiles are reversed for the Clayton copula."],"metadata":{"id":"zQyJlCaJzfRV"}},{"cell_type":"code","source":["BestCop.plot()\n","ClaytonCop.plot()"],"metadata":{"id":"4UQo9FzqORdC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2D contour plot of PDF. Again, the quantiles are reversed for the Clayton copula."],"metadata":{"id":"OJqo9tptS-dl"}},{"cell_type":"code","source":["BestCop.plot(type=\"contour\")\n","ClaytonCop.plot(type=\"contour\")"],"metadata":{"id":"dTGAw8VyOFEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Repeat with color for the Joe Copula."],"metadata":{"id":"NAhPGtHQTAm8"}},{"cell_type":"code","source":["import matplotlib as mpl\n","\n","# compute the PDF over a grid of u-space values\n","x, y = np.mgrid[0:1.01:0.01, 0:1.01:0.01]\n","coordinates = np.stack((x,y), axis=-1).reshape(-1, 2)\n","prob = BestCop.pdf(coordinates).reshape(x.shape)\n","\n","# create a linear segmented colormap\n","cmap = mpl.colormaps.get_cmap(\"viridis\")\n","cmaplist = [cmap(i) for i in range(cmap.N)]\n","cmap = mpl.colors.LinearSegmentedColormap.from_list(\n","    'Custom cmap', cmaplist, cmap.N)\n","\n","# define the contour levels and normalize the colormap to those levels\n","levels = [0,0.2,0.6,1,1.5,2,3,5,50]\n","norm = mpl.colors.BoundaryNorm(levels, cmap.N)\n","\n","# creat the contour map\n","colors = plt.contourf(x, y, prob, cmap=cmap, norm=norm, levels=levels)\n","plt.scatter(u_D, u_S, c='k')\n","plt.xlim(0,1)\n","plt.ylim(0,1)\n","cbar = plt.colorbar(colors)\n","cbar.set_label('Probability Density')\n","plt.xlabel('u_D')\n","plt.ylabel('u_S')\n","plt.title('Joe Copula')\n","plt.show()"],"metadata":{"id":"0hTKQN2JziiF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And for the Clayton copula. With our own function, we can reverse the quantiles so it's more interpretable."],"metadata":{"id":"wbhjprUy0-UB"}},{"cell_type":"code","source":["prob = ClaytonCop.pdf(coordinates).reshape(x.shape)\n","\n","# creat the contour map\n","colors = plt.contourf(x, y, prob, cmap=cmap, norm=norm, levels=levels)\n","plt.scatter(1-u_D, 1-u_S, c='k')\n","plt.xlim(0,1)\n","plt.ylim(0,1)\n","cbar = plt.colorbar(colors)\n","cbar.set_label('Probability Density')\n","plt.xlabel('1-u_D')\n","plt.ylabel('1-u_S')\n","plt.title('Clayton Copula')\n","plt.show()"],"metadata":{"id":"9Gn_kSLT1D7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Repeat for the CDF."],"metadata":{"id":"0ah0KJBySn80"}},{"cell_type":"code","source":["cumprob = BestCop.cdf(coordinates).reshape(x.shape)\n","\n","colors = plt.contourf(x, y, cumprob, levels=np.arange(0,1.1,0.1))\n","plt.scatter(u_D, u_S, c='k')\n","plt.xlim(0,1)\n","plt.ylim(0,1)\n","cbar = plt.colorbar(colors)\n","cbar.set_label('Cumulative Probability')\n","plt.xlabel('u_D')\n","plt.ylabel('u_S')\n","plt.title('Joe Copula')\n","plt.show()"],"metadata":{"id":"Qi7hU-BiSpmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And for the Clayton copula."],"metadata":{"id":"WoyCvQBf1P94"}},{"cell_type":"code","source":["cumprob = ClaytonCop.cdf(coordinates).reshape(x.shape)\n","\n","colors = plt.contourf(x, y, cumprob, levels=np.arange(0,1.1,0.1))\n","plt.scatter(1-u_D, 1-u_S, c='k')\n","plt.xlim(0,1)\n","plt.ylim(0,1)\n","cbar = plt.colorbar(colors)\n","cbar.set_label('Cumulative Probability')\n","plt.xlabel('1-u_D')\n","plt.ylabel('1-u_S')\n","plt.title('Clayton Copula')\n","plt.show()"],"metadata":{"id":"GNg2ILRs1RnD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Computing Return Periods from Fitted Copula\n","\n","Compute the return period of the most severe event using the Joe Copula."],"metadata":{"id":"cZZogmRbbkPe"}},{"cell_type":"code","source":["OR_return = 1 / (1 - BestCop.cdf(np.array([u[np.argmax(u_S),:]]))[0])\n","AND_return = 1 / (1 - u_D[np.argmax(u_S)] - u_S[np.argmax(u_S)] + BestCop.cdf(np.array([u[np.argmax(u_S),:]]))[0])\n","\n","print(\"OR return period: %0.2f\" % OR_return)\n","print(\"AND return period: %0.2f\" % AND_return)"],"metadata":{"id":"TVjwdwDrbwIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now using the Clayton copula. Since we fit the copula to 1-u, we'd want to know the probability of being below, not above that value in one or both variables."],"metadata":{"id":"Yz5Sb2MJ17YS"}},{"cell_type":"code","source":["AND_return = 1 / (BestCop.cdf(np.array([1-u[np.argmax(u_S),:]]))[0])\n","OR_return = 1 / ((1-u_D[np.argmax(u_S)]) + (1-u_S[np.argmax(u_S)]) - BestCop.cdf(np.array([1-u[np.argmax(u_S),:]]))[0])\n","\n","print(\"OR return period: %0.2f\" % OR_return)\n","print(\"AND return period: %0.2f\" % AND_return)"],"metadata":{"id":"IO7_AJI9192z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This fit suggests that seeing a drought that is more severe than the most severe drought in terms of both duration and severity is much more unusual."],"metadata":{"id":"wT2fLLSQ3eFQ"}},{"cell_type":"markdown","source":["## Computing Conditional Distributions from Fitted Copula\n","\n","Find the conditional distribution of drought severity given it lasts 3 years and compare with the unconditional distribution of severity using the Joe Copula."],"metadata":{"id":"9YP-RYrvbpmm"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","u1 = np.mean(u_D[np.where(durations==3)[0]])\n","\n","x = np.linspace(0,15,1000)\n","u2 = ss.lognorm.cdf(x, distfit.sigma, distfit.tau, np.exp(distfit.mu))\n","cond_u_S = np.empty(1000)\n","for i in range(len(u2)):\n","  cond_u_S[i] = BestCop.hfunc1(np.array([[u1,u2[i]]])) # function to get P(u2<u2 | u1=u1)\n","\n","# plot conditional vs. unconditional CDF\n","fig, ax = plt.subplots()\n","ax.plot(x, cond_u_S, label='CDF given duration=3')\n","ax.plot(x, ss.lognorm.cdf(x, distfit.sigma, distfit.tau, np.exp(distfit.mu)), label='Unconditional CDF')\n","ax.legend()\n","ax.set_xlabel(\"Drought Severity\")\n","ax.set_ylabel(\"Cumulative Probability\")"],"metadata":{"id":"2vLBvrxybw5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Repeat for the Clayton copula."],"metadata":{"id":"0tcoqxrt4OMV"}},{"cell_type":"code","source":["cond_u_S = np.empty(1000)\n","for i in range(len(u2)):\n","  cond_u_S[i] = BestCop.hfunc1(np.array([[u1,u2[i]]])) # function to get P(u2<u2 | u1=u1)\n","\n","fig, ax = plt.subplots()\n","ax.plot(x, cond_u_S, label='CDF given duration=3')\n","ax.plot(x, ss.lognorm.cdf(x, distfit.sigma, distfit.tau, np.exp(distfit.mu)), label='Unconditional CDF')\n","ax.legend()\n","ax.set_xlabel(\"Drought Severity\")\n","ax.set_ylabel(\"Cumulative Probability\")"],"metadata":{"id":"aYnp63rT4oT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These look remarkably similar. Let's plot the conditional distributions from each fit together."],"metadata":{"id":"KN4GLiXA5INu"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","\n","cond_u_S_Joe = np.empty(1000)\n","for i in range(len(u2)):\n","  cond_u_S_Joe[i] = BestCop.hfunc1(np.array([[u1,u2[i]]])) # function to get P(u2<u2 | u1=u1)\n","\n","ax.plot(x, cond_u_S_Joe, label='Conditional CDF with JoeCop')\n","\n","cond_u_S_Clayton = np.empty(1000)\n","for i in range(len(u2)):\n","  cond_u_S_Clayton[i] = 1-ClaytonCop.hfunc1(np.array([[1-u1,1-u2[i]]])) # function to get P(u2<u2 | u1=u1)\n","\n","# plot conditional vs. unconditional CDF\n","ax.plot(x, cond_u_S_Clayton, label='Conditional CDF with ClaytonCop')\n","ax.legend()\n","ax.set_xlabel(\"Drought Severity\")\n","ax.set_ylabel(\"Cumulative Probability\")"],"metadata":{"id":"z9sXgg9i4QDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["They are very close! What does each suggest is the conditional and unconditional probability that the severity is at least 5 given the duration is 3 years?"],"metadata":{"id":"XFRS3hxnMcu4"}},{"cell_type":"code","source":["p_Uncond_5 = 1 - ss.lognorm.cdf(5, distfit.sigma, distfit.tau, np.exp(distfit.mu))\n","p_Cond_5_Joe = cond_u_S_Joe[np.argmin(np.abs(x-5))]\n","p_Cond_5_Clayton = cond_u_S_Clayton[np.argmin(np.abs(x-5))]\n","\n","print(\"Unconditional probability severity >= 5: %0.2f\" % p_Uncond_5)\n","print(\"Probability severity >= 5 given duration = 3: %0.2f\" % p_Cond_5_Joe)\n","print(\"Probability severity >= 5 given duration = 3: %0.2f\" % p_Cond_5_Clayton)"],"metadata":{"id":"x7cZgsqhMyJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simulating from Fitted Copula\n","\n","Generate 1000 drought durations and severities from the Joe copula and plot the simulated vs. observed values."],"metadata":{"id":"pZtX7mCybuBb"}},{"cell_type":"code","source":["uSim = BestCop.simulate(1000, qrng=True, seeds=[1])\n","\n","durSim = np.zeros(1000)\n","sevSim = np.zeros(1000)\n","for i in range(1000):\n","  durSim[i] = durations[np.argmin(np.abs(uSim[i,0]-u_D))]\n","  sevSim[i] = ss.lognorm.ppf(uSim[i,1], distfit.sigma, distfit.tau, np.exp(distfit.mu))\n","\n","plt.scatter(durSim, sevSim, c='r', label='Simulated')\n","plt.scatter(durations, severities, c='k', label='Observed')\n","plt.legend()\n","plt.xlabel('Drought Durations')\n","plt.ylabel('Drought Severities')\n","plt.show()"],"metadata":{"id":"uEXKZzYobxVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare marginals"],"metadata":{"id":"dwBF2ER_wofh"}},{"cell_type":"code","source":["plt.hist(durSim, alpha=0.5, label='Simulated', color=\"tab:blue\", density=True)\n","plt.hist(durations, alpha=0.5, label='Observed', color=\"tab:green\", density=True)\n","plt.legend()\n","plt.xlabel('Drought Durations')\n","plt.ylabel('Density')\n","plt.show()"],"metadata":{"id":"_zaXYAaLvVdG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These are identical because we used the empirical distribution of drought duration."],"metadata":{"id":"YvT9iApe6GuY"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","sns.kdeplot(sevSim, color=\"tab:blue\", ax=ax)\n","sns.kdeplot(severities, color=\"tab:green\", ax=ax)\n","ax.legend(labels=['Simulated', 'Observed'])\n","ax.set_xlabel('Drought Severities')\n","ax.set_ylabel('Density')\n","plt.show()"],"metadata":{"id":"wq5Fg8PiwPLm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While there are some much more severe droughts generated than observed, the marginal distributions are in fact very close, so this is not unreasonable for generating 1000 droughts instead of the observed 112.\n","\n","Repeat for the Clayton Copula"],"metadata":{"id":"wjGhtrQO5rwF"}},{"cell_type":"code","source":["uSim = ClaytonCop.simulate(1000, qrng=True, seeds=[1])\n","# reverse quantiles\n","uSim = 1 - uSim\n","\n","durSim = np.zeros(1000)\n","sevSim = np.zeros(1000)\n","for i in range(1000):\n","  durSim[i] = durations[np.argmin(np.abs(uSim[i,0]-u_D))]\n","  sevSim[i] = ss.lognorm.ppf(uSim[i,1], distfit.sigma, distfit.tau, np.exp(distfit.mu))\n","\n","plt.scatter(durSim, sevSim, c='r', label='Simulated')\n","plt.scatter(durations, severities, c='k', label='Observed')\n","plt.legend()\n","plt.xlabel('Drought Durations')\n","plt.ylabel('Drought Severities')\n","plt.show()"],"metadata":{"id":"YYxKg3HO5tAX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(durSim, alpha=0.5, label='Simulated', color=\"tab:blue\", density=True)\n","plt.hist(durations, alpha=0.5, label='Observed', color=\"tab:green\", density=True)\n","plt.legend()\n","plt.xlabel('Drought Durations')\n","plt.ylabel('Density')\n","plt.show()"],"metadata":{"id":"IQiU4cvX6C-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, this matches perfectly because we used the empirical distribution of drought duration."],"metadata":{"id":"3hg-QW7s6cJV"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","sns.kdeplot(sevSim, color=\"tab:blue\", ax=ax)\n","sns.kdeplot(severities, color=\"tab:green\", ax=ax)\n","ax.legend(labels=['Simulated', 'Observed'])\n","ax.set_xlabel('Drought Severities')\n","ax.set_ylabel('Density')\n","plt.show()"],"metadata":{"id":"8GAVGCQi6gbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, while we simulated some droughts with much greater severity, the distribution is in fact reasonable."],"metadata":{"id":"4Tr0h_s16rN7"}}]}