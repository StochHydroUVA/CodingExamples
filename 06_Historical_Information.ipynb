{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBGn/Kw6pGG9mFhhRtOnS5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Incorporating Historical Information into Flood Frequency Analysis"],"metadata":{"id":"TgWnGOpqbhGp"}},{"cell_type":"markdown","source":["Here we'll show how to incorporate historical flood information into flood frequency analysis, combining a systematic gauged record at [USGS gauge 09357500](https://waterdata.usgs.gov/nwis/inventory?site_no=09357500&agency_cd=USGS), the Animas River at Howardsville, Colorado. This gauge has a 47-year systematic record from WY 1936-1982.  \n","\n","In [England et al., 2003](https://www.sciencedirect.com/science/article/pii/S0022169403001410?casa_token=5mUAFml5gLwAAAAA:4Bz--Z0WNxOWyN0a-FD4XuTbBPW6YiOMwtvuYtW-SLU6tmJkgJH3bxKrFzhn6AXN-9TzrbFziww), they note a historical peak of 2470 cfs at this location 953 years ago. These are the threshold and historical record length. We'll see how this information influences our estimate of the LP3 parameters and 100-year flood at this location.\n","\n","We'll use the `dataretrieval` library to download this data again, but this time we'll just download the annual maxima (`service='peaks'`) rather than the daily values (`service='dv'`)."],"metadata":{"id":"r9b_i6BRzESa"}},{"cell_type":"code","source":["!pip install dataretrieval"],"metadata":{"id":"CEkDvCLmbQEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import dataretrieval.nwis as nwis\n","\n","flow_df = nwis.get_record(sites='09357500', service='peaks', start='1936-05-25', end='1982-06-28') # Animas River at Howardsville, CO\n","flow_df.head()"],"metadata":{"id":"NRqxtvQxbqP7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All we care about is the `peak_va` column, so let's just select that."],"metadata":{"id":"VIcqLERQ0hcO"}},{"cell_type":"code","source":["flow_df = flow_df[\"peak_va\"]\n","flow_df.head()"],"metadata":{"id":"2hYtS6Fqc260"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To fit an LP3 distribution, first import the classes and functions from `utils.py`. Since this uses the `lmoments3` library, we'll need to install that here as well."],"metadata":{"id":"GGJMKodwZePj"}},{"cell_type":"code","source":["!pip install lmoments3"],"metadata":{"id":"Euj7E0jrZJif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# allow access to google drive\n","drive.mount('/content/drive')\n","\n","!cp \"drive/MyDrive/Colab Notebooks/CE6280/CodingExamples/utils.py\" .\n","from utils import *"],"metadata":{"id":"-ye5yV60ZOtk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For your future homeworks, you can include the code below within `utils.py`, but I'm pasting it below rather than including it there so `utils.py` does not contain the answers to your first homework ðŸ˜‰.\n","\n","Below I've included a function to estimate the parameters of the Gamma distribution using both systematic and historical information with the Expected Moments Algorithm (EMA). This functions can be included within `utils.py` for your future homeworks as well. You'll also need to write a similar function to estimate the parameters with the Approximate Moments Algorithm (AMA)."],"metadata":{"id":"hJ1SSYD4Zvwp"}},{"cell_type":"code","source":["import scipy.stats as ss\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class Gamma(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.alpha = None\n","    self.xi = None\n","    self.beta = None\n","\n","  def fit(self, data, method, npars, initialize=True, hist_data=None, h=None, thres=None, params0 = None, tolerance=None, maxiter=None):\n","    assert method == 'MLE' or method == 'MOM' or method == \"EMA\",\"method must = 'MLE', 'MOM' or 'EMA'\"\n","    assert npars == 2 or npars == 3,\"npars must = 2 or 3\"\n","\n","    self.findMoments(data)\n","    if method == 'MLE':\n","      if initialize == False:\n","        if npars == 2:\n","          shape, loc, scale = ss.gamma.fit(data, floc=0)\n","        elif npars == 3:\n","          shape, loc, scale = ss.gamma.fit(data)\n","      else:\n","        if npars == 2:\n","          self.fit(data, 'MOM', 2)\n","          shape, loc, scale = ss.gamma.fit(data, self.alpha, floc=0)\n","        elif npars == 3:\n","          self.fit(data, 'MOM', 3)\n","          shape, loc, scale = ss.gamma.fit(data, self.alpha)\n","\n","      self.alpha = shape\n","      self.xi = loc\n","      self.beta = 1/scale\n","    elif method == 'MOM':\n","      if npars == 2:\n","        self.alpha = self.xbar**2/self.var\n","        self.beta = self.xbar/self.var\n","        self.xi = 0\n","      elif npars == 3:\n","        self.alpha = 4/self.skew**2\n","        self.beta = np.sqrt(self.alpha/self.var)\n","        self.xi = self.xbar - self.alpha/self.beta\n","    elif method == 'EMA':\n","      x = data\n","      y = hist_data\n","      s = len(x)\n","\n","      # Get vector of all peaks over the threshold and find how many there are in the historical record\n","      y_prime = np.concatenate((y[np.where(y >= thres)], x[np.where(x >= thres)]),0)\n","      k = len(np.where(y >= thres)[0])\n","\n","      # Get vector of all observed peaks below the threshold (those during the systematic record, the last s years)\n","      # Find how many peaks in the systematic record, m, exceeded the threshold\n","      x_prime = x[np.where(x < thres)]\n","      m = s - len(x_prime)\n","\n","      # Fit the distribution parameters using EMA\n","      # Step 1: estimate initial parameters alpha_hat, beta_hat, xi_hat\n","      # with MOM using only systematic record\n","      #self.fit(x, \"MOM\", 3)\n","      alpha_hat = params0.alpha\n","      beta_hat = params0.beta\n","      xi_hat = params0.xi\n","\n","      # Step 2: Use initial parameter estimates to find expected moments below the threshold.\n","      # Then compute \"new\" sample moments based on observed floods and expected moments of unseen floods\n","      c2 = (s + k) / (s + k - 1)\n","      c3 = (s + k)**2 / ((s + k - 1)*(s + k - 2))\n","      for i in range(maxiter):\n","        # find expected value below the threshold with current parameter estimates\n","        D = ss.gamma(alpha_hat)\n","        if beta_hat > 0:\n","          E_xH_below = ss.gamma.expect(lambda x: x, D.args, loc=xi_hat, scale=1/beta_hat, ub=thres, conditional=True)\n","        else:\n","          E_xH_below = -ss.gamma.expect(lambda x: x, D.args, loc=-xi_hat, scale=-1/beta_hat, lb=-thres, conditional=True)\n","\n","        # update estimate of the mean\n","        mu_new = (np.sum(x_prime) + np.sum(y_prime) + (h-k)*E_xH_below) / (s + h)\n","\n","        # find expected value of x^2 below the threshold with current parameter estimates\n","        if beta_hat > 0:\n","          E_xH_below_2 = ss.gamma.expect(lambda x: (x-mu_new)**2, D.args, loc=xi_hat, scale=1/beta_hat, ub=thres, conditional=True)\n","        else:\n","          E_xH_below_2 = ss.gamma.expect(lambda x: (x+mu_new)**2, D.args, loc=-xi_hat, scale=-1/beta_hat, lb=-thres, conditional=True)\n","\n","        # update estimate of the standard deviation\n","        sigma_new = np.sqrt((c2 * (np.sum((x_prime - mu_new)**2) + np.sum((y_prime - mu_new)**2)) + (h-k)*E_xH_below_2) / (s+h))\n","\n","        # find expected value of x^3 below the threshold with current parameter estimates\n","        if beta_hat > 0:\n","          E_xH_below_3 = ss.gamma.expect(lambda x: (x-mu_new)**3, D.args, loc=xi_hat, scale=1/beta_hat, ub=thres, conditional=True)\n","        else:\n","          E_xH_below_3 = -ss.gamma.expect(lambda x: (x+mu_new)**3, D.args, loc=-xi_hat, scale=-1/beta_hat, lb=-thres, conditional=True)\n","\n","        # update estimate of the skewness\n","        gamma_new = ((c3*(np.sum((x_prime - mu_new)**3) + np.sum((y_prime - mu_new)**3)) + (h-k)*E_xH_below_3)) / ((s+h)*sigma_new**3)\n","\n","        # Step 3: Use new moments to find corresponding \"new\" estimates of alpha, xi and beta\n","        alpha_new = 4/gamma_new**2\n","        beta_new = (np.abs(gamma_new)/gamma_new) * np.sqrt(alpha_new)/sigma_new\n","        xi_new = mu_new - alpha_new/beta_new\n","\n","        # calculate difference between old (\"hat\") and \"new\" parameter estimates\n","        alphaDiff = 0.5 * np.abs(( alpha_new - alpha_hat) / (alpha_new + alpha_hat))\n","        xiDiff = 0.5 * np.abs((xi_new - xi_hat) / (xi_new + xi_hat))\n","        betaDiff = 0.5 * np.abs((beta_new - beta_hat) / (beta_new + beta_hat))\n","        totalDiff = alphaDiff +  xiDiff + betaDiff\n","\n","        # update old (\"hat\") estimates with \"new\" estimates\n","        alpha_hat = alpha_new\n","        beta_hat = beta_new\n","        xi_hat = xi_new\n","\n","        # Step 4: convergence test\n","        # exit loop if total difference between past and current estimates is within the tolerance\n","        if totalDiff < tolerance:\n","          break\n","\n","      # return parameter estimates from EMA loop\n","      self.alpha = alpha_hat\n","      self.xi = xi_hat\n","      self.beta = beta_hat\n","\n","  def findReturnPd(self, T):\n","    q_T = ss.gamma.ppf(1-1/T, self.alpha, self.xi, 1/self.beta)\n","    return q_T\n","\n","  def plotHistPDF(self, data, min, max, title):\n","    x = np.arange(min, max,(max-min)/100)\n","    f_x = ss.gamma.pdf(x, self.alpha, self.xi, 1/self.beta)\n","\n","    plt.hist(data, density=True)\n","    plt.plot(x,f_x)\n","    plt.xlim([min, max])\n","    plt.title(title)\n","    plt.xlabel('Flow')\n","    plt.ylabel('Probability Density')\n","    plt.show()"],"metadata":{"id":"_z1fASXsY1h7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What would be the parameter and 100-year flood estimates with just the systematic record using LP3 with MOM?"],"metadata":{"id":"uQIZlpFVhLNl"}},{"cell_type":"code","source":["dist = Gamma()\n","if ss.skew(np.log(flow_df),bias=False) > 0:\n","  dist.fit(np.log(flow_df), \"MOM\", 3)\n","  q100 = np.exp(dist.findReturnPd(100))\n","  dist.plotHistPDF(np.log(flow_df), 6, 8, \"LP3 MOM Fit\")\n","else:\n","  dist.fit(-np.log(flow_df), \"MOM\", 3)\n","  q100 = np.exp(-dist.findReturnPd(1/0.99))\n","  dist.plotHistPDF(-np.log(flow_df), -8, -6, \"LP3 MOM Fit\")\n","\n","print(\"alpha: %0.2f\" % dist.alpha)\n","print(\"xi: %0.2f\" % dist.xi)\n","print(\"beta: %0.2f\" % dist.beta)\n","print(\"q100: %0.0f cfs\" % q100)"],"metadata":{"id":"iolSAunIhJ14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How does this change using LP3 with EMA and the historical flood?"],"metadata":{"id":"kz4tHAf8hQvm"}},{"cell_type":"code","source":["hist_data = np.array([2470])\n","thres = 2470\n","\n","params0 = Gamma()\n","if ss.skew(np.log(flow_df),bias=False) < 0:\n","  params0.fit(-np.log(flow_df), \"MOM\", 3)\n","  params0.beta = -params0.beta\n","  params0.xi = -params0.xi\n","else:\n","  params0.fit(np.log(flow_df), \"MOM\", 3)\n","\n","dist = Gamma()\n","dist.fit(np.log(np.array(flow_df)), \"EMA\", 3, True, np.log(hist_data), 953, np.log(thres), params0, tolerance=0.0001, maxiter=100)\n","if dist.beta > 0:\n","  q100 = np.exp(dist.findReturnPd(100))\n","  dist.plotHistPDF(np.log(flow_df), 6, 8, \"EMA MOM Fit\")\n","  T_hist = 1 / (1 - ss.gamma.cdf(np.log(2470), dist.alpha, dist.xi, 1/dist.beta))\n","else:\n","  dist.beta = -dist.beta\n","  dist.xi = -dist.xi\n","  q100 = np.exp(-dist.findReturnPd(1/0.99))\n","  dist.plotHistPDF(-np.log(flow_df), -8, -6, \"EMA MOM Fit\")\n","  T_hist = 1 / ss.gamma.cdf(-np.log(2470), dist.alpha, dist.xi, 1/dist.beta)\n","\n","print(\"alpha: %0.2f\" % dist.alpha)\n","print(\"xi: %0.2f\" % dist.xi)\n","print(\"beta: %0.2f\" % dist.beta)\n","print(\"q100: %0.0f cfs\" % q100)\n","print(\"Return period of historical flood: %0.0f years\" % T_hist)"],"metadata":{"id":"vRjC8I3ZgOxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The 100-yr flood estimate shifts downward from 2023 cfs to 1977 cfs. Other locations might see more of a shift from historical information. The historical flood of 2470 cfs that was observed 953 years ago has an estimated return period of 1029 years, not far from its empirical estimate given the record length."],"metadata":{"id":"1Ps700_MaNT9"}}]}