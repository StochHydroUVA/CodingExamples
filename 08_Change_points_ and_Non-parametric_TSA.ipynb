{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrf7B3RzlvcvX1vIYPu990"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Testing for change points with regression, and non-parametric tests for trends and change points"],"metadata":{"id":"ZA56NzzVAndz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2cSa-fxmr9a"},"outputs":[],"source":["!pip install dataretrieval"]},{"cell_type":"markdown","source":["Load annual maxima from the Turkey River in Garber, Iowa. This is USGS site [05412500](https://waterdata.usgs.gov/nwis/inventory?site_no=05412500&agency_cd=USGS), which has data from Aug 8, 1913 to the present, but is only complete from 1932-10-01, so we'll load the data from then on."],"metadata":{"id":"egiYg-PGH2kh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import dataretrieval.nwis as nwis\n","\n","flow_df = nwis.get_record(sites='05412500', service='peaks', start='1932-10-01', end='2024-09-30') # Turkey River at Garber, IA\n","flow_df.head()"],"metadata":{"id":"tXrOpFDNH3Js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flow_df[\"peak_va\"].plot()"],"metadata":{"id":"Ca2oxusiIajd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing for Normality"],"metadata":{"id":"XQo5sFKlJ1lS"}},{"cell_type":"code","source":["!pip install lmoments3"],"metadata":{"id":"sPKHvbjhJ3ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# allow access to google drive\n","drive.mount('/content/drive')\n","\n","!cp \"drive/MyDrive/Colab Notebooks/CE6280/CodingExamples/utils.py\" .\n","from utils import *"],"metadata":{"id":"KWgIMho0J7Rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import scipy.stats as ss\n","import matplotlib.pyplot as plt\n","from lmoments3 import distr\n","\n","class Normal(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","    self.sigma = None\n","\n","  def fit(self, data, method):\n","    assert method == 'MLE' or method == 'MOM' or method == 'Lmom',\"method must = 'MLE', 'MOM' or 'Lmom'\"\n","\n","    self.findMoments(data)\n","    if method == 'MLE':\n","      self.mu, self.sigma = ss.norm.fit(data)\n","    elif method == 'MOM':\n","      self.mu = self.xbar\n","      self.sigma = np.sqrt(self.var)\n","    elif method == 'Lmom':\n","      norm_params = distr.nor.lmom_fit(data)\n","      self.mu = norm_params[\"loc\"]\n","      self.sigma = norm_params[\"scale\"]\n","\n","  def findReturnPd(self, T):\n","    q_T = ss.norm.ppf(1-1/T, self.mu, self.sigma)\n","    return q_T\n","\n","  def plotHistPDF(self, data, min, max, title):\n","    x = np.arange(min, max,(max-min)/100)\n","    f_x = ss.lognorm.pdf(x, self.mu, self.sigma)\n","    self.plotDistFit(data, x, f_x, min, max, title)\n","\n","  def ppccTest(self, data, title, m=10000):\n","    # calculate test statistic, rho\n","    x_sorted = np.sort(data)\n","    p_observed = ss.mstats.plotting_positions(x_sorted)\n","    x_fitted = ss.norm.ppf(p_observed, self.mu, self.sigma)\n","    self.ppcc_rho = np.corrcoef(x_sorted, x_fitted)[0,1]\n","\n","    # generate m synthetic samples of n observations to estimate null distribution of rho\n","    rhoVector = np.zeros(m)\n","    for i in range(m):\n","      np.random.seed(i)\n","      x = ss.norm.rvs(self.mu, self.sigma, size=len(data))\n","      rhoVector[i] = np.corrcoef(np.sort(x), x_fitted)[0,1]\n","\n","    # calculate p-value of test and make QQ plot\n","    count = 0\n","    for i in range(len(rhoVector)):\n","      if self.ppcc_rho < rhoVector[i]:\n","        count = count + 1\n","\n","    self.p_value_PPCC = 1 - count/(len(rhoVector) + 1)\n","\n","    # make Q-Q plot\n","    plt.scatter(x_sorted,x_fitted,color='b')\n","    plt.plot(x_sorted,x_sorted,color='r')\n","    plt.xlabel('Observations')\n","    plt.ylabel('Fitted Values')\n","    plt.title(title)\n","    plt.show()\n","\n","  def calcCI(self, data, p, CI, method, npars, seed):\n","    n = len(data)\n","    alpha = (100.0-CI)/100.0\n","    # calculate theoretical confidence interval using formula from slides\n","    z_p = ss.norm.ppf(p)\n","    z_crit = ss.norm.ppf(1-alpha/2)\n","    x_p = self.mu + z_p*self.sigma\n","    LB = x_p - z_crit * np.sqrt(self.sigma**2 * (1+0.5*z_p**2)/n)\n","    UB = x_p + z_crit * np.sqrt(self.sigma**2 * (1+0.5*z_p**2)/n)\n","    return LB, UB"],"metadata":{"id":"vQi4MAjTJ7x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dist = Normal()\n","dist.fit(flow_df[\"peak_va\"], \"MLE\")\n","dist.ppccTest(flow_df[\"peak_va\"], \"Normal Fit\")\n","dist.p_value_PPCC"],"metadata":{"id":"JgtVltmfKBo9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What if we use a log-transformation?"],"metadata":{"id":"jtPn6qlTKXHq"}},{"cell_type":"code","source":["flow_df[\"logQ\"] = np.log(flow_df[\"peak_va\"])\n","\n","dist = Normal()\n","dist.fit(flow_df[\"logQ\"], \"MLE\")\n","dist.ppccTest(flow_df[\"logQ\"], \"Normal Fit\")\n","dist.p_value_PPCC"],"metadata":{"id":"TpCI3RGLKZ-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That looks pretty good and we don't reject that the data is normal! Let's proceed we that.\n","\n","## Looking for seasonality\n","\n","We'll look for seasonality using the periodogram."],"metadata":{"id":"9QYDs_zKKji1"}},{"cell_type":"code","source":["from scipy.signal import periodogram\n","\n","# look at periodogram of transformed annual maxima\n","f, P = periodogram(flow_df[\"logQ\"])\n","plt.plot(f, P)\n","plt.xlabel('Frequency')\n","plt.ylabel('Squared Amplitude')\n","\n","# what are the periods of the largest values?\n","Psorted = np.argsort(P)[::-1] # sort from largest to smallest\n","1/f[Psorted] # find corresponding periods"],"metadata":{"id":"uVsPPiznKorp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most prominent peak corresponds to a period of 4.8 years, followed by a period of 2.5 years. There's no reason to believe annual maxima would have seasonality at exactly those periods; this could be noise or elements of ENSO, but that will be more cyclic than seasonal. So we'll assume there is no seasonality.\n","\n","## Looking for Autocorrelation"],"metadata":{"id":"ewR-EBYMLVXI"}},{"cell_type":"code","source":["import statsmodels.api as sm\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(2,1,1)\n","sm.graphics.tsa.plot_acf(flow_df[\"logQ\"],ax=ax)\n","ax.set_xlim([0,10])\n","\n","ax = fig.add_subplot(2,1,2)\n","sm.graphics.tsa.plot_pacf(flow_df[\"logQ\"],ax = ax)\n","ax.set_xlim([0,10])\n","\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"6nXJEMDnL7ob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is no significant auto-correlation we need to control for either! So we can go straight to testing for a trend.\n","\n","## Testing for a trend"],"metadata":{"id":"IccvexOAMJ-U"}},{"cell_type":"code","source":["X = np.array([np.ones(len(flow_df[\"logQ\"])), range(1933,2025)]).T\n","y = flow_df[\"logQ\"]\n","\n","trend_model = sm.OLS(y, X)\n","trend_result = trend_model.fit()\n","print(trend_result.summary())"],"metadata":{"id":"WVnFN63xNXTB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The trend is not statistically significant. What does it look like?"],"metadata":{"id":"UEntcqU4Oki_"}},{"cell_type":"code","source":["l1, = plt.plot(flow_df[\"logQ\"], color=\"tab:blue\")\n","l2, = plt.plot(trend_result.fittedvalues, color=\"tab:red\")\n","plt.legend([l1,l2],['True Values','Predictions'], loc='upper left')"],"metadata":{"id":"AoF2aX1-Ons6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check the regression assumptions of independence, normality, and constant variance of residuals.\n","\n","### Independence of Residuals"],"metadata":{"id":"dhhU6HsCgXu8"}},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(2,1,1)\n","sm.graphics.tsa.plot_acf(trend_result.resid, ax=ax)\n","ax.set_xlim([0,20])\n","ax.set_ylim([-1.2,1.2])\n","\n","ax = fig.add_subplot(2,1,2)\n","sm.graphics.tsa.plot_pacf(trend_result.resid, ax=ax)\n","ax.set_xlim([0,20])\n","ax.set_ylim([-1.2,1.2])\n","\n","fig.tight_layout()\n","\n","fig.show()"],"metadata":{"id":"D3KDfJcFgfZi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks good - we have removed all autocorrelation (there wasn't any to begin with, so not surprising).\n","\n","### Normality of Residuals"],"metadata":{"id":"DRW1eRknitYH"}},{"cell_type":"code","source":["# qq plot of residuals vs. normal fit\n","sm.qqplot(trend_result.resid,ss.norm,fit=True,line='45')"],"metadata":{"id":"AYm5BZfrisy2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks good as well. By making the data normal, we ensured the residuals were as well.\n","\n","### Constant Variance of Residuals"],"metadata":{"id":"mI2YsJYUiuUW"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","sns.residplot(x=trend_result.fittedvalues, y=trend_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})"],"metadata":{"id":"J6i45bf2it7u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This isn't bad. The mean is relatively constant at 0 (red line is close to 0 throughout). There is a little more variance in the residuals at lower fitted values, though."],"metadata":{"id":"NNmB3V_KpXTd"}},{"cell_type":"markdown","source":["# Non-Parametric Trend Test\n","\n","We were actually able to meet the assumptions of regression fairly well, but what if we did not? Then we'd have to use the Mann-Kendall non-parametric trend test. We can get a function for this from the [pymannkendall](https://pypi.org/project/pymannkendall/) library. Examples on how to use this library can be found [here](https://github.com/mmhs013/pyMannKendall/blob/master/Examples/Example_pyMannKendall.ipynb). We'll show it on this data for illustrative purposes."],"metadata":{"id":"WRm2LgTLPAXu"}},{"cell_type":"code","source":["!pip install pymannkendall"],"metadata":{"id":"lidsutMrPD69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pymannkendall as mk\n","mk.original_test(flow_df[\"logQ\"], alpha=0.05)"],"metadata":{"id":"ggeFsT-EPkYb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This also says there is no statistically significant trend. How about a change point?\n","\n","## Change Point Detection\n","\n","### For change in mean via regression"],"metadata":{"id":"Knl1y5v0Pzop"}},{"cell_type":"code","source":["import statsmodels.formula.api as smf\n","\n","# create a new columns of 0/1 depending on whether it is before or after time T\n","# loop through different values of T and find p-value of dummy variable\n","# find T that minimizes p-value --> most likely change point\n","flow_df['Dummy'] = np.ones(len(flow_df['logQ']))\n","p_values = np.ones(len(flow_df['logQ'])-1)\n","\n","# loop through each time step, changing the dummy to 0 one-at-a-time\n","# fit the regression model each time and record the p-value\n","for i in range(len(flow_df['logQ'])-1):\n","  flow_df.loc[flow_df.index[i],\"Dummy\"] = 0\n","  mod = smf.ols(formula='logQ ~ Dummy',data=flow_df)\n","  result = mod.fit()\n","  p_values[i] = result.pvalues[\"Dummy\"]\n","\n","l1, = plt.plot(range(1934,1934+len(flow_df['logQ'])-1),p_values,c='tab:red')\n","l2, = plt.plot([1934,1934+len(flow_df['logQ'])-1],[0.05,0.05],c='k')\n","plt.legend([l1,l2],['p-values','0.05 threshold'],bbox_to_anchor=(0.5, 0.15),loc='center')\n","plt.xlabel('Water Year')\n","plt.ylabel('p-value')\n","plt.title('Change Point Detection')\n","\n","print(\"Minimum p-value: %0.2f\" % np.min(p_values))\n","print(\"Most likely change point: %d\" % (1934 + np.argmin(p_values)))"],"metadata":{"id":"FlRETij9QKyu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The p-value is never below 0.05, so there is no location with a statistically significant change point in the mean of the annual maxima."],"metadata":{"id":"u0rKEJ3KrhBq"}},{"cell_type":"markdown","source":["### For change in median with Wilcoxon rank sum test"],"metadata":{"id":"EZK3fumyQPrV"}},{"cell_type":"code","source":["# test for changepoint with Wilcoxon rank-sum test\n","p_values = np.ones(len(flow_df[\"logQ\"])-1)\n","for i in range(len(flow_df[\"logQ\"])-1):\n","  D, p_values[i] = ss.ranksums(flow_df[\"logQ\"].iloc[:(i+1)], flow_df[\"logQ\"].iloc[(i+1):])\n","\n","l1, = plt.plot(range(1934,1934+len(flow_df['logQ'])-1),p_values,c='tab:red')\n","l2, = plt.plot([1934,1934+len(flow_df['logQ'])-1],[0.05,0.05],c='k')\n","plt.legend([l1,l2],['p-values','0.05 threshold'],bbox_to_anchor=(0.5, 0.15),loc='center')\n","plt.xlabel('Water Year')\n","plt.ylabel('p-value')\n","plt.title('Change Point Detection')\n","\n","print(\"Minimum p-value: %0.2f\" % np.min(p_values))\n","print(\"Most likely change point: %d\" % (1934 + np.argmin(p_values)))"],"metadata":{"id":"knhFebSDQTIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are a few locations where the p-value dips below 0.05. The lowest p-value occurs in 2013, indicating this is the most likely time of a change in the median of the annual maxima."],"metadata":{"id":"3FdSxgS9tnkd"}},{"cell_type":"markdown","source":["### For change in distribution with K-S test"],"metadata":{"id":"5S9TO7W_QYW9"}},{"cell_type":"code","source":["# test for changepoint with K-S test\n","p_values = np.ones(len(flow_df[\"logQ\"])-1)\n","for i in range(len(flow_df[\"logQ\"])-1):\n","  D, p_values[i] = ss.ks_2samp(flow_df[\"logQ\"].iloc[:(i+1)], flow_df[\"logQ\"].iloc[(i+1):])\n","\n","l1, = plt.plot(range(1934,1934+len(flow_df['logQ'])-1),p_values,c='tab:red')\n","l2, = plt.plot([1934,1934+len(flow_df['logQ'])-1],[0.05,0.05],c='k')\n","plt.legend([l1,l2],['p-values','0.05 threshold'],bbox_to_anchor=(0.5, 0.15),loc='center')\n","plt.xlabel('Water Year')\n","plt.ylabel('p-value')\n","plt.title('Change Point Detection')\n","\n","print(\"Minimum p-value: %0.2f\" % np.min(p_values))\n","print(\"Most likely change point: %d\" % (1934 + np.argmin(p_values)))"],"metadata":{"id":"q3ub5ZmbQau0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are several locations where the p-value dips below 0.05. The lowest p-value occurs in 2013, indicating this is the most likely time of a change in the distribution of the annual maxima."],"metadata":{"id":"YW270TVIuTSn"}},{"cell_type":"markdown","source":["### For change in mean with bootstrapping"],"metadata":{"id":"QjueBMw3QfIS"}},{"cell_type":"code","source":["!pip install astropy"],"metadata":{"id":"tGU47s2KUIOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from astropy.stats import bootstrap as bootstrap\n","\n","# test for changepoint with bootstrapping\n","nSamples=1000\n","p_values = np.ones(len(flow_df[\"logQ\"])-1)\n","for i in range(len(flow_df[\"logQ\"])-1):\n","  np.random.seed(i+1)\n","  x = bootstrap(np.array(flow_df[\"logQ\"].iloc[:(i+1)]),nSamples)\n","  y = bootstrap(np.array(flow_df[\"logQ\"].iloc[(i+1):]),nSamples)\n","  S = 0\n","  for j in range(nSamples):\n","    if np.mean(x[j,:]) > np.mean(y[j,:]):\n","      S+=1\n","\n","  if S/nSamples > 0.5:\n","    p_values[i] = 2*(1-S/nSamples)\n","  else:\n","    p_values[i] = 2*S/nSamples\n","\n","l1, = plt.plot(range(1934,1934+len(flow_df['logQ'])-1),p_values,c='tab:red')\n","l2, = plt.plot([1934,1934+len(flow_df['logQ'])-1],[0.05,0.05],c='k')\n","plt.legend([l1,l2],['p-values','0.05 threshold'],bbox_to_anchor=(0.5, 0.15),loc='center')\n","plt.xlabel('Water Year')\n","plt.ylabel('p-value')\n","plt.title('Change Point Detection')\n","\n","print(\"Minimum p-value: %0.2f\" % np.min(p_values))\n","print(\"Most likely change point: %d\" % (1934 + np.argmin(p_values)))"],"metadata":{"id":"BRxjHTHxQhot"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["According to bootstrapping, the most likely change in the mean occurs in 1934, but there's only one year before that, so there isn't really enough data before then to be confident in that conclusion."],"metadata":{"id":"MNSChhqUwVOX"}},{"cell_type":"markdown","source":["### For change in variance with bootstrapping"],"metadata":{"id":"2Pb9a76JQjp2"}},{"cell_type":"code","source":["# is the variance changing?\n","nSamples=1000\n","p_values = np.ones(len(flow_df[\"logQ\"])-1)\n","for i in range(len(flow_df[\"logQ\"])-1):\n","  np.random.seed(i+1)\n","  x = bootstrap(np.array(flow_df[\"logQ\"].iloc[:(i+1)]),nSamples)\n","  y = bootstrap(np.array(flow_df[\"logQ\"].iloc[(i+1):]),nSamples)\n","  S = 0\n","  for j in range(nSamples):\n","    if np.var(x[j,:]) > np.var(y[j,:]):\n","      S+=1\n","\n","  if S/nSamples > 0.5:\n","    p_values[i] = 2*(1-S/nSamples)\n","  else:\n","    p_values[i] = 2*S/nSamples\n","\n","l1, = plt.plot(range(1934,1934+len(flow_df['logQ'])-1),p_values,c='tab:red')\n","l2, = plt.plot([1934,1934+len(flow_df['logQ'])-1],[0.05,0.05],c='k')\n","plt.legend([l1,l2],['p-values','0.05 threshold'],bbox_to_anchor=(0.5, 0.15),loc='center')\n","plt.xlabel('Water Year')\n","plt.ylabel('p-value')\n","plt.title('Change Point Detection')\n","\n","print(\"Minimum p-value: %0.2f\" % np.min(p_values))\n","print(\"Most likely change point: %d\" % (1934 + np.argmin(p_values)))"],"metadata":{"id":"8djy8j0ZQl4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The most likely change point in the variance is again 1934, but there is no variance before that because there is only one data point to sample. This is true at the last year as well. There are several years in between that also show a statistically significant difference before and after, so there isn't an obvious single change point."],"metadata":{"id":"fw4fzmQ6efjE"}}]}