{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5xe48Rp0fIijSBHAKBUjI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Time Series Analysis: Controlling for Seasonality and Auto-correlation"],"metadata":{"id":"vU0tJhilqOtf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LDaF8AYmn00"},"outputs":[],"source":["!pip install dataretrieval"]},{"cell_type":"markdown","source":["Load data from the Turkey River in Garber, Iowa. This is USGS site [05412500](https://waterdata.usgs.gov/nwis/inventory?site_no=05412500&agency_cd=USGS), which has data from Aug 8, 1913 to the present."],"metadata":{"id":"u94aJ2zpxs--"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import dataretrieval.nwis as nwis\n","\n","flow_df = nwis.get_record(sites='05412500', service='dv', parameterCd='00060', start='1913-10-01', end='2024-09-30') # Turkey River at Garber, IA\n","flow_df.head()"],"metadata":{"id":"B1xlDBubqw0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the data"],"metadata":{"id":"MwJihSiZxeYy"}},{"cell_type":"code","source":["flow_df[\"00060_Mean\"].plot()"],"metadata":{"id":"Qw6FFf6jwwbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are clearly periods of missing data, which is a problem for time series analysis. These periods are too long to simply interpolate, so we will start our analysis after their is a consistent record. What are the missing periods?"],"metadata":{"id":"On9oaSW6y9Cm"}},{"cell_type":"code","source":["flow_df[\"00060_Mean\"].isna().any()"],"metadata":{"id":"Mg2jG77pza0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flow_df[\"00060_Mean\"].isnull().any()"],"metadata":{"id":"z07SbyV-B_y3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's not that the data is NA or null, the dates are just missing."],"metadata":{"id":"-RWQWHZfCG82"}},{"cell_type":"code","source":["dates  = pd.date_range(start=flow_df.index.min(), end=flow_df.index.max())\n","len(dates)"],"metadata":{"id":"PuAVI1TMCV3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(flow_df.index)"],"metadata":{"id":"yE_RFHjJCgYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find which dates are missing."],"metadata":{"id":"JAbbsIJrDcsz"}},{"cell_type":"code","source":["dates[np.where(dates.isin(flow_df.index) == False)[0]]"],"metadata":{"id":"exhGI6rfDb5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data is complete after 1932-10-1, so we'll start there."],"metadata":{"id":"PBjezqQvDuud"}},{"cell_type":"code","source":["flow_df = flow_df.loc[\"1932-10-01\":]\n","flow_df.head()"],"metadata":{"id":"vkO1_S3RDyZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test if data is normally distributed."],"metadata":{"id":"5Ge19SzdEU7U"}},{"cell_type":"code","source":["!pip install lmoments3"],"metadata":{"id":"kepV6_gDHKB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# allow access to google drive\n","drive.mount('/content/drive')\n","\n","!cp \"drive/MyDrive/Colab Notebooks/CE6280/CodingExamples/utils.py\" .\n","from utils import *"],"metadata":{"id":"SQw6T9lvGgKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import scipy.stats as ss\n","import matplotlib.pyplot as plt\n","from lmoments3 import distr\n","\n","class Normal(Distribution):\n","  def __init__(self):\n","    super().__init__()\n","    self.mu = None\n","    self.sigma = None\n","\n","  def fit(self, data, method):\n","    assert method == 'MLE' or method == 'MOM' or method == 'Lmom',\"method must = 'MLE', 'MOM' or 'Lmom'\"\n","\n","    self.findMoments(data)\n","    if method == 'MLE':\n","      self.mu, self.sigma = ss.norm.fit(data)\n","    elif method == 'MOM':\n","      self.mu = self.xbar\n","      self.sigma = np.sqrt(self.var)\n","    elif method == 'Lmom':\n","      norm_params = distr.nor.lmom_fit(data)\n","      self.mu = norm_params[\"loc\"]\n","      self.sigma = norm_params[\"scale\"]\n","\n","  def findReturnPd(self, T):\n","    q_T = ss.norm.ppf(1-1/T, self.mu, self.sigma)\n","    return q_T\n","\n","  def plotHistPDF(self, data, min, max, title):\n","    x = np.arange(min, max,(max-min)/100)\n","    f_x = ss.lognorm.pdf(x, self.mu, self.sigma)\n","    self.plotDistFit(data, x, f_x, min, max, title)\n","\n","  def ppccTest(self, data, title, m=10000):\n","    # calculate test statistic, rho\n","    x_sorted = np.sort(data)\n","    p_observed = ss.mstats.plotting_positions(x_sorted)\n","    x_fitted = ss.norm.ppf(p_observed, self.mu, self.sigma)\n","    self.ppcc_rho = np.corrcoef(x_sorted, x_fitted)[0,1]\n","\n","    # generate m synthetic samples of n observations to estimate null distribution of rho\n","    rhoVector = np.zeros(m)\n","    for i in range(m):\n","      np.random.seed(i)\n","      x = ss.norm.rvs(self.mu, self.sigma, size=len(data))\n","      rhoVector[i] = np.corrcoef(np.sort(x), x_fitted)[0,1]\n","\n","    # calculate p-value of test and make QQ plot\n","    count = 0\n","    for i in range(len(rhoVector)):\n","      if self.ppcc_rho < rhoVector[i]:\n","        count = count + 1\n","\n","    self.p_value_PPCC = 1 - count/(len(rhoVector) + 1)\n","\n","    # make Q-Q plot\n","    plt.scatter(x_sorted,x_fitted,color='b')\n","    plt.plot(x_sorted,x_sorted,color='r')\n","    plt.xlabel('Observations')\n","    plt.ylabel('Fitted Values')\n","    plt.title(title)\n","    plt.show()\n","\n","  def calcCI(self, data, p, CI, method, npars, seed):\n","    n = len(data)\n","    alpha = (100.0-CI)/100.0\n","    # calculate theoretical confidence interval using formula from slides\n","    z_p = ss.norm.ppf(p)\n","    z_crit = ss.norm.ppf(1-alpha/2)\n","    x_p = self.mu + z_p*self.sigma\n","    LB = x_p - z_crit * np.sqrt(self.sigma**2 * (1+0.5*z_p**2)/n)\n","    UB = x_p + z_crit * np.sqrt(self.sigma**2 * (1+0.5*z_p**2)/n)\n","    return LB, UB"],"metadata":{"id":"uLqX2fK1HHHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dist = Normal()\n","dist.fit(flow_df[\"00060_Mean\"], \"MLE\")\n","dist.ppccTest(flow_df[\"00060_Mean\"], \"Normal Fit\")\n","dist.p_value_PPCC"],"metadata":{"id":"cZRlcDm2Hl25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, the fit is terrible! Let's try a Box-Cox transformation."],"metadata":{"id":"DnHhUTuZIG-A"}},{"cell_type":"code","source":["flow_df[\"Q_transformed\"], boxcox_power = ss.boxcox(flow_df[\"00060_Mean\"])\n","\n","dist = Normal()\n","dist.fit(flow_df[\"Q_transformed\"], \"MLE\")\n","dist.ppccTest(flow_df[\"Q_transformed\"], \"Normal Fit\")\n","dist.p_value_PPCC"],"metadata":{"id":"tM67WA__IKQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We still reject, but it's much better, so we'll use this moving forward. Below we plot this transformed time series."],"metadata":{"id":"6Zy95EQMIrDN"}},{"cell_type":"code","source":["flow_df[\"Q_transformed\"].plot()"],"metadata":{"id":"zB66kqTKcFgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It looks like it might have an increasing trend, but first let's remove seasonality and auto-correlation. We'll start by looking for seasonality in the transformed time series with the periodogram."],"metadata":{"id":"AuBzHqJ1cF5I"}},{"cell_type":"code","source":["from scipy.signal import periodogram\n","\n","f, P = periodogram(flow_df[\"Q_transformed\"])\n","plt.semilogx(f, P)\n","plt.xlabel('Frequency')\n","plt.ylabel('Amplitude Squared')"],"metadata":{"id":"OKN1V6I3It8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can clearly see periodicity associated with the annual cycle and its harmonics."],"metadata":{"id":"_vtoP0z3JLPh"}},{"cell_type":"code","source":["plt.plot([1/365.25, 1/365.25],[-50,1100],color='r',linestyle=\"--\")\n","plt.plot([2/365.25, 2/365.25],[-50,1100],color='r',linestyle=\"--\")\n","plt.plot([3/365.25, 3/365.25],[-50,1100],color='r',linestyle=\"--\")\n","plt.plot([4/365.25, 4/365.25],[-50,1100],color='r',linestyle=\"--\")\n","plt.semilogx(f, P)\n","plt.xlabel('Frequency')\n","plt.ylabel('Amplitude Squared')"],"metadata":{"id":"XmyCNT81JShe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The first three are fairly significant, but the fourth is within the noise of other peaks. So let's just include three harmonics in our regression model.  \n","There are multiple ways you can set up an OLS regression in Python. We'll use notation similar to R here where you pass a formula: `y ~ x1 + x2 + ...` where `y` is the response variable and the terms after the tilde are the predictors, all of which have to be in the same data frame."],"metadata":{"id":"nk6jyM-_H4lv"}},{"cell_type":"code","source":["import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","\n","# add columns for time (which we'll later use to test for a trend)\n","# and the first three harmonics of annual seasonality\n","flow_df['t'] = np.arange(len(flow_df.index))\n","for i in range(3):\n","  flow_df['sine' + str(i+1)] = np.sin(2*np.pi*(i+1)*flow_df['t']/365.25)\n","  flow_df['cosine' + str(i+1)] = np.cos(2*np.pi*(i+1)*flow_df['t']/365.25)\n","\n","season_model = smf.ols(formula=\"Q_transformed ~  sine1 + cosine1 + sine2 + cosine2 + sine3 + cosine3\", data=flow_df)\n","season_result = season_model.fit()\n","print(season_result.summary())"],"metadata":{"id":"f2FvaSCtIBO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All predictors and the model are statistically significant.  \n","\n","Now check to see if we have \"removed\" seasonality by looking at the periodogram of the residuals."],"metadata":{"id":"vNMvJjpDXQR2"}},{"cell_type":"code","source":["f, P = periodogram(season_result.resid)\n","plt.semilogx(f, P)\n","plt.xlabel('Frequency')\n","plt.ylabel('Amplitude Squared')"],"metadata":{"id":"-jZYd3aOXI-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The only peaks that stand out now correspond to lower frequencies, which we might be able to capture with autocorrelation, so we'll model that next with ARIMA models. First, let's see what our new time series looks like after removing seasonality."],"metadata":{"id":"WdbedIbdXj4L"}},{"cell_type":"code","source":["plt.plot(season_result.resid)"],"metadata":{"id":"AA32Wzw7cTFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modeling seasonality didn't change the appearance much. It still looks liek there might be a trend, but let's see if that could be from auto-correlation.  \n","To hypothesize an appropriate ARMA model, let's look at the ACF and PACF of the new time series."],"metadata":{"id":"Ie7vUwxfcTcB"}},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(2,1,1)\n","sm.graphics.tsa.plot_acf(season_result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","ax = fig.add_subplot(2,1,2)\n","sm.graphics.tsa.plot_pacf(season_result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"ASsQDaescO_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["An AR(1) model may be appropriate, as the ACF seems to decay exponentially and only the first lag is very significant in the PACF. Let's see if an AR(1) model sufficiently captures the auto-correlation."],"metadata":{"id":"PfgKakX3csv_"}},{"cell_type":"code","source":["from statsmodels.tsa.ar_model import AutoReg\n","\n","AR1model = AutoReg(season_result.resid, lags=1)\n","AR1result = AR1model.fit()\n","print(AR1result.summary())"],"metadata":{"id":"wSzEitQncyEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(2,1,1)\n","sm.graphics.tsa.plot_acf(AR1result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","ax = fig.add_subplot(2,1,2)\n","sm.graphics.tsa.plot_pacf(AR1result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"Qjuxxypj0PoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This seems to have removed most of the auto-correlation!  \n","\n","For comparison, we can also use automated selection with the [auto.arima function](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.AutoARIMA.html#pmdarima.arima.AutoARIMA) in the [pmdarima](https://pypi.org/project/pmdarima/) library, but that can be slow for large datasets, so we'll reserve an example for a different time series and stick with the simple AR(1) model for now since it seems to do well.  \n","\n","Because we've removed seasonality and auto-correlation, we can now test for a trend in the residuals. Let's look at the time series we're modeling now."],"metadata":{"id":"4y1Xp4Es0gTh"}},{"cell_type":"code","source":["plt.plot(AR1result.resid)"],"metadata":{"id":"2uI4X0lvch8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Removing auto-correlation greatly changed the look of the time series! Now it's not very clear if there is a trend in the mean, although there does seem to be a decrease in variability.  \n","\n","To test for a trend in this time series, we'll build an OLS model to the residuals of the AR(1) model with time as a predictor. Below we use matrix notation instead of R's formula notation."],"metadata":{"id":"JLSUkh0FemlC"}},{"cell_type":"code","source":["X = np.array([np.ones(len(AR1result.resid)), range(len(AR1result.resid))]).T\n","y = AR1result.resid\n","\n","trend_model = sm.OLS(y, X)\n","trend_result = trend_model.fit()\n","print(trend_result.summary())"],"metadata":{"id":"7MzVRyAV02Xj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is a statistically significant positive trend, though it is slight. What does the trend look like?"],"metadata":{"id":"5VoQgEHBfTsl"}},{"cell_type":"code","source":["l1, = plt.plot(AR1result.resid, color=\"tab:blue\")\n","l2, = plt.plot(trend_result.fittedvalues, color=\"tab:red\")\n","plt.legend([l1,l2],['True Values','Predictions'], loc='upper right')"],"metadata":{"id":"uczThksqfUeC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly the trend is small in magnitude, even if it is statistically significant!  \n","\n","For perspective, let's convert this daily trend back to real-space and see how many cfs it corresponds to every 365.25 days (year) for the average flow value. Note, because the Box-Cox transformation is non-linear, the trend in real-space is also nonlinear and therefore depends on the flow. For greater interpretability, we can compute that increase as a percentage of the average daily flow.  \n","\n","Remember the formula for a Box-Cox transformation of $Q$ to $Q_T$:\n","\n","$$Q_T = \\frac{Q^\\lambda - 1}{\\lambda}$$  \n","$$Q_T = \\frac{1}{\\lambda}Q^\\lambda - \\frac{1}{\\lambda}$$\n","\n","We calculated the slope of this Box-Cox transformed value over time, $\\frac{\\partial Q_T}{\\partial t}$ with our OLS model above. We can convert that to the slope of the real-space values over time, $\\frac{\\partial Q}{\\partial t}$, using calculus:  \n","\n","$$\\frac{\\partial Q_T}{\\partial t} = Q^{\\lambda-1}\\frac{\\partial Q}{\\partial t}$$  \n","$$\\frac{\\partial Q}{\\partial t} = Q^{1-\\lambda}\\frac{\\partial Q_T}{\\partial t}$$  \n","\n","As you can see, the slope in real space depends on the value of the flow $Q$, making it nonlinear. We'll plug in the average flow $\\bar{Q}$ for $Q$, yielding a real-space slope for the mean daily flow of:  \n","\n","$$\\frac{\\partial Q}{\\partial t} = \\bar{Q}^{1-\\lambda}\\frac{\\partial Q_T}{\\partial t}$$."],"metadata":{"id":"hUX4jsdm4jp5"}},{"cell_type":"code","source":["# average increase in transformed daily flow per year\n","slope_per_year = trend_result.params[\"x1\"]*365.25\n","\n","# transform back to real-space for the mean daily flow\n","slope_per_year = np.mean(flow_df[\"00060_Mean\"])**(1-boxcox_power) * slope_per_year\n","\n","# calculate this as a percentage of the mean daily flow\n","percent_per_year = 100*slope_per_year/np.mean(flow_df[\"00060_Mean\"])\n","\n","print(\"Trend in average daily flow evaluated at the mean flow: %0.2f cfs/year\" % slope_per_year)\n","print(\"This is %0.2f%% of the mean daily flow\" % percent_per_year)"],"metadata":{"id":"BO4xfd4D4ttf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Caveats:\n","\n","When using OLS regression to test for a trend, we need to make sure the residuals are independent and normally distributed, with constant variance. We can look for independence by plotting the ACF of the residuals of our final model. We can look for normality on a QQ-plot, and we can look for constant variance in a residuals vs. fitted plot. We do that below.\n","\n","### Testing for independence"],"metadata":{"id":"6T8izfd1_VPu"}},{"cell_type":"code","source":["fig = plt.figure()\n","ax = fig.add_subplot(2,1,1)\n","sm.graphics.tsa.plot_acf(trend_result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","ax = fig.add_subplot(2,1,2)\n","sm.graphics.tsa.plot_pacf(trend_result.resid, ax=ax)\n","ax.set_xlim([0,30])\n","ax.set_ylim([-1.2,1.2])\n","\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"2iT63pAY_zRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks pretty good, how about normality?\n","\n","### Testing for normality"],"metadata":{"id":"aLf-BsJM_7rj"}},{"cell_type":"code","source":["# qq plot of residuals vs. normal fit\n","sm.qqplot(trend_result.resid,ss.norm,fit=True,line='45')"],"metadata":{"id":"pkYGnVvT-qVO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks pretty bad! How about constant variance?\n","\n","### Residuals vs. fitted to look for constant variance."],"metadata":{"id":"66KBlYZnABZa"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","sns.residplot(x=trend_result.fittedvalues, y=trend_result.resid, lowess=True,\n","              scatter_kws={'alpha': 0.5},\n","              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})"],"metadata":{"id":"biAsNjQy6xTp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is also not great (the variance is decreasing), and there is not much more we can do because we already transformed our data with a Box-Cox transformation.  \n","\n","When you can't meet the assumptions of regression, it's good to also consider non-parametric trend tests that don't require these assumptions. We'll learn those next lecture, and also look for change points in time series, not just trends."],"metadata":{"id":"AWZdHqqDAK06"}}]}